---
format: revealjs
title: Regression Measurements
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```

```{python}
from sklearn.model_selection import train_test_split

housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)
```

<br>

```{python}
# | include: false
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 
```

```{python}
X_train = train_df.drop(columns=["median_house_value"])
y_train = train_df["median_house_value"]
X_test = test_df.drop(columns=["median_house_value"])
y_test = test_df["median_house_value"]

numeric_features = [ "longitude", "latitude",
                     "housing_median_age",
                     "households", "median_income",
                     "rooms_per_household",
                     "bedrooms_per_household",
                     "population_per_household"]
                     
categorical_features = ["ocean_proximity"]

X_train.head()
```



:::{.notes} 
This next section involves looking at regression problems and so we are going to bring back our California housing dataset where we want to predict the median house value for different locations.
:::

---

```{python}
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import make_column_transformer
from sklearn.neighbors import KNeighborsRegressor

numeric_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="median")), 
           ("scaler", StandardScaler())]
)

categorical_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
           ("onehot", OneHotEncoder(handle_unknown="ignore"))]
)

preprocessor = make_column_transformer(
(numeric_transformer, numeric_features),
        (categorical_transformer, categorical_features), 
    remainder='passthrough')

pipe = make_pipeline(preprocessor, KNeighborsRegressor())
pipe.fit(X_train, y_train);
```


:::{.notes} 
We are going to bring in our previous pipelines and fit our model. 
:::

---

```{python}
predicted_y = pipe.predict(X_train) 
```

<br>

```{python}
predicted_y == y_train
```

<br>

```{python}
y_train.values
```

<br>

```{python}
predicted_y
```


:::{.notes} 
We aren't doing classification anymore, so we can't just check for equality. 

We need a score that reflects how right/wrong each prediction is or how close we are to the actual numeric value.
:::

---

## Regression measurements 

The scores we are going to discuss are: 

- mean squared error (MSE)
- R<sup>2</sup>
- root mean squared error (RMSE)
- MAPE


If you want to see these in more detail, you can refer to the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics" target="_blank">sklearn documentation</a>.


---

## Mean squared error (MSE)

![](../../../static/module7/mse.svg){fig-align="center" width="20%" fig-alt="404 image"}

![](../../../static/module7/mse-easy.svg){fig-align="center" width="38%" fig-alt="404 image"}

```{python}
predicted_y
```

<br>

```{python}
np.mean((y_train - predicted_y)**2)
```

<br>
```{python}
np.mean((y_train - y_train)**2)
```


:::{.notes} 
Mean Squared Error is a common measure.

We calculate this by calculating the difference between the predicted and actual value, square it and sum all these values for every example in the data. 

Perfect predictions would have MSE=0.
:::

---


```{python}
from sklearn.metrics import mean_squared_error 
```

<br>

```{python}
mean_squared_error(y_train, predicted_y)
```


:::{.notes} 
We can use `mean_squared_error` from sklearn again instead of calculating this ourselves. 

If we look at MSE here, it's huge and unreasonable. 

Is this score good or bad?

Unlike classification, in regression, our target has units. 

In this case, our target column is the median housing value which is in dollars. 

That means that the mean squared error is in dollars<sup>2</sup>. 

The score also depends on the scale of the targets. 

If we were working in cents instead of dollars, our MSE would be 10,000 X (100<sup>2</sup>) higher!
:::

---

## R<sup>2</sup> (quick notes)

Key points:

- The maximum value possible is 1 which means the model has perfect predictions.
- Negative values are very bad: "worse than baseline models such as`DummyRegressor`". 


```{python}
from sklearn.metrics import r2_score
```


:::{.notes}
This is the score that `sklearn` uses by default when you call `.score()` so we've already seen R<sup>2</sup> in our regression problems. 

You can <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank">read about it here</a> but we are going to just give you the quick notes. 

Intuition: mean squared error, but flipped where higher values mean a better measurement.

Normalized so the max is 1.

When you call `fit` it minimizes MSE / maximizes R<sup>2</sup> (or something like that) by default.

Just like in classification, this isn't always what you want.
:::

---

```python
print(mean_squared_error(y_train, predicted_y))
print(mean_squared_error(predicted_y, y_train))
```

```out
2570054492.048064
2570054492.048064
```

<br>

```python
print(r2_score(y_train, predicted_y))
print(r2_score(predicted_y, y_train))
```

```out
0.8059396097446094
0.742915970464153
```


:::{.notes} 
We can reverse MSE but not R<sup>2</sup> (optional).
:::

---

## Root mean squared error  (RMSE)

![](../../../static/module7/rmse-simp.svg){fig-align="center" width="20%" fig-alt="404 image"}

![](../../../static/module7/rmse-easy.svg){fig-align="center" width="38%" fig-alt="404 image"}


```{python}
mean_squared_error(y_train, predicted_y)
```

<br>

```{python}
np.sqrt(mean_squared_error(y_train, predicted_y))
```



:::{.notes} 
The MSE we had before was in dollars<sup>2</sup>.

A more relatable metric would be the root mean squared error, or RMSE. 

This now has the units in dollars.  Instead of 2 billion dollars squared our error measurement is around $50,000.
:::

---

```{python}
# | echo: false
# | fig-align: center
import matplotlib.pyplot as plt

df = pd.DataFrame(y_train).assign(predicted = predicted_y).rename(columns = {'median_house_value': 'true'})
df = pd.DataFrame(y_train).assign(predicted = predicted_y).rename(columns = {'median_house_value': 'true'})
plt.scatter(y_train, predicted_y, alpha=0.3, s = 10)
grid = np.linspace(y_train.min(), y_train.max(), 1000)
plt.plot(grid, grid, '--k');
plt.xticks(fontsize= 12);
plt.yticks(fontsize= 12);
plt.xlabel("true price", fontsize=14);
plt.ylabel("predicted price", fontsize=14)
```


:::{.notes} 
When we plot our predictions versus the examples' actual value, we can see cases where our prediction is way off.

Under the line means we're under-prediction, over the line means we're over-predicting.

Question: Is an error of $30,000 acceptable?     

- For a house worth $600k, it seems reasonable! That's a 5% error.
- For a house worth $60k, that is terrible. It's a 50% error.
:::

---

## MAPE - Mean Absolute Percent Error (MAPE)

```{python}
percent_errors = (predicted_y - y_train)/y_train * 100.
percent_errors.head()
```

<br>

```{python}
np.abs(percent_errors).head()
```

<br>

```{python}
100.*np.mean(np.abs((predicted_y - y_train)/y_train))
```


:::{.notes} 
So, finding the percentage error may be handy. Can we compute something like that?

We can calculate a percentage error for each example. Now the errors are both positive (predict too high) and negative (predict too low).

We can look at the absolute percent error which now shows us how far off we were independent of direction. 

Like MSE, we can take the average over all the examples. This is called **Mean Absolute Percent Error (MAPE)**.

Ok, this is quite interpretable. We can see that on average, we have around 18% error in our predicted median housing valuation.
:::


# Letâ€™s apply what we learned!