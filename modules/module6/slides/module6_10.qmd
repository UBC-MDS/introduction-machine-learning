---
format: revealjs
title: ColumnTransformer
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```

```{python}
# | include: false
from sklearn.model_selection import train_test_split

housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)
 
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 
 

X_train = train_df.drop(columns=["median_house_value"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value"])
y_test = test_df["median_house_value"]
```

## Problem: We have different transformations for different columns

Before we fit our model, we want to apply different transformations on different columns.

- Numeric columns:
    - imputation 
    - scaling  
    
- Categorical columns:   
    - imputation      
    - one-hot encoding    
    

:::{.notes} 
We can't use a pipeline since not all the transformations are occurring on every feature. 

We could do so without but then we would be violating the Golden Rule of Machine learning when we did cross-validation.

So we need a new tool and it's called `ColumnTransformer`!
:::

---

## *ColumnTransformer*

<br>

![](../../../static/module6/column-transformer.png){fig-align="center" width="90%" fig-alt="404 image"}

 <a href="https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#37" target="_blank">Adapted from here. </a>

:::{.notes} 
sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html" target="_blank">`ColumnTransformer`</a> makes this more manageable.

A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data. 

Otherwise, we might, for example, do the OHE on both train and test but forget to scale the test data.  
:::

---

```{python}
from sklearn.compose import ColumnTransformer
```

<br>

```{python}
X_train.head()
```

:::{.notes} 
We import `ColumnTransformer` from the `sklearn` library. 

And we will have to look at our data.
:::

---

```{python}
X_train.dtypes
```

<br>

```{python}
numeric_features = [ "longitude",
                     "latitude",
                     "housing_median_age",
                     "households",
                     "median_income",
                     "rooms_per_household",
                     "bedrooms_per_household",
                     "population_per_household"]
                     
categorical_features = ["ocean_proximity"]
```


:::{.notes} 
We must first identify the categorical and numeric columns.
:::

---

```{python}
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

numeric_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="median")), 
           ("scaler", StandardScaler())]
)

categorical_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
           ("onehot", OneHotEncoder(handle_unknown="ignore"))]
)
```


:::{.notes} 
Next, we build a pipeline for our dataset.

This means we need to make at least 2 preprocessing pipelines; one for the categorical and one for the numeric features! 

(If we needed to use the ordinal encoder for binary data or ordinal features then we would need a third.)
:::

---

```{python}
col_transformer = ColumnTransformer(
    transformers=[
        ("numeric", numeric_transformer, numeric_features),
        ("categorical", categorical_transformer, categorical_features)
    ], 
    remainder='passthrough'    
)
```

:::{.notes} 
We then call the numeric and categorical features with their respective transformers in `ColumnTransformer()`.

The `ColumnTransformer` syntax is somewhat similar to that of `Pipeline` in that you pass in a list of tuples.

But, this time, each tuple has 3 values instead of 2: (name of the step, transformer object, list of columns)

A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data.

Otherwise, we might, for example, do the OHE on both train and test but forget to scale the test data.

`remainder="passthrough"`:   

- The `ColumnTransformer` will automatically remove columns that are not being transformed.      
- We can use `remainder="passthrough"` of `ColumnTransformer` to keep the other columns intact.      

We don't have any columns that are being removed in this case but this is a good feature to have if we are only interested in a few features.
:::

---

```{python}
col_transformer.fit(X_train)
```


:::{.notes} 
When we `fit` with the `col_transformer`, it calls `fit` on ***all*** the transformers.

And when we transform with the preprocessor, it calls `transform` on ***all*** the transformers.
:::

---

```{python}
X_train.head()
```

<br>

```{python}
x = list(X_train.columns.values)
del x[5]
X_train_pp = col_transformer.transform(X_train)
pd.DataFrame(X_train_pp, columns= (x  + list(col_transformer.named_transformers_["categorical"].named_steps["onehot"].get_feature_names_out(categorical_features)))).head()
```


:::{.notes} 
Here we can see what our dataframe looks like after transformation. 
:::

---

```{python}
onehot_cols = col_transformer.named_transformers_["categorical"].named_steps["onehot"].get_feature_names_out(categorical_features)
onehot_cols
```

<br>

```{python}
columns = numeric_features + list(onehot_cols)
columns
```


:::{.notes} 
We can get the new names of the columns that were generated by the one-hot encoding.

Combining this with the numeric feature names gives us all the column names.
:::

---

```{python}
from sklearn.neighbors import KNeighborsRegressor

main_pipe = Pipeline(
    steps=[
        ("preprocessor", col_transformer), # <-- this is the ColumnTransformer!
        ("reg", KNeighborsRegressor())])
```

<br>

```{python}
from sklearn.model_selection import cross_validate

with_categorical_scores = cross_validate(main_pipe, X_train, y_train, return_train_score=True)
pd.DataFrame(with_categorical_scores)
```

:::{.notes} 
Now we use a main pipeline to transform all the data and build a model. 

Scaling and one hot encoding are now applied at the same time!

We can then use `cross_validate()` and find our mean training and validation scores!
:::

---

```{python}
pipe = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
        ("reg", KNeighborsRegressor())])
        
pipe.fit(X_train.drop(columns=['ocean_proximity']), y_train);
```

<br>

```{python}
no_categorical_scores = cross_validate(pipe, X_train.drop(columns=['ocean_proximity']), y_train, return_train_score=True)
pd.DataFrame(no_categorical_scores)
```


:::{.notes} 
Let's compare what we did before without ColumTransformer and without categorical columns and observe how adding the `ocean_proximity` column changes our results. 
:::

---

```{python}
pd.DataFrame(no_categorical_scores).mean()
```

<br>

```{python}
pd.DataFrame(with_categorical_scores).mean()
```


:::{.notes} 
We can see here that adding and one hot encoding our `ocean_proximity` column improves our score. 

This was a single column. 

If we had more columns, we could improve our scores in a much more substantial way instead of throwing the information away which is what we have been doing!
:::

---

```{python}
from sklearn import set_config

set_config(display='diagram')
main_pipe
```


:::{.notes} 
Since there are a lot of steps happening we can use `set_config` from sklearn and it will display a diagram of what is going on in our main pipeline. 
:::

---


![](../../../static/module6/pipeline.png){fig-align="center" width="90%" fig-alt="404 image"}


:::{.notes} 
We can also look at this image which shows the more generic version of what happens in `ColumnTransformer` and where it stands in our main pipeline.
:::

---


### Do we need to preprocess categorical values in the target column?

- Generally, there is no need for this when doing classification. 
- `sklearn` is fine with categorical labels (y-values) for classification problems. 



# Letâ€™s apply what we learned!
