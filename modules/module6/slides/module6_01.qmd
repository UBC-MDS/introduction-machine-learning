---
format: revealjs
title: "Categorical Variables: Ordinal Encoding"
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```

## Remember our case study with the California housing dataset? 

```{python}
# | include: false
pd.set_option('display.max_rows', 15)
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)
 
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 
 
X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]
 
X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
 
imputer = SimpleImputer(strategy="median")
imputer.fit(X_train)
X_train_imp = imputer.transform(X_train)
X_test_imp = imputer.transform(X_test) 
```


```{python}
train_df.head()
```

<br>

```{python}
X_train = train_df.drop(columns=["median_house_value"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value"])
y_test = test_df["median_house_value"]
```


:::{.notes} 
Remember in module 5, we preprocessed only the numeric variables of our California housing dataset. 

Early on, before we even did imputation, we dropped the categorical feature `ocean_proximity` feature from the dataframe. 

We just discussed how dropping certain columns is not always the best idea since we could be dropping potentially useful features in this task. 

Categorical variables can be extremely useful in that they require their own different kind of preprocessing. 

Let's create our `X_train` and and `X_test` again by keeping the `ocean_proximity` feature in the data this time.
:::

---

```{python}
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
pipe = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
        ("reg", KNeighborsRegressor()),
    ]
)
```

```python
pipe.fit(X_train, y_train)
```

```out
ValueError: Cannot use median strategy with non-numeric data:
could not convert string to float: 'INLAND'

Detailed traceback: 
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.8/site-packages/sklearn/pipeline.py", line 330, in fit
    Xt = self._fit(X, y, **fit_params_steps)
  File "/usr/local/lib/python3.8/site-packages/sklearn/pipeline.py", line 292, in _fit
    X, fitted_transformer = fit_transform_one_cached(
  File "/usr/local/lib/python3.8/site-packages/joblib/memory.py", line 352, in __call__
    return self.func(*args, **kwargs)
```


:::{.notes} 
Let's first see what happens when we try to apply a ùëò-NN model on our data and preprocess it for imputation and scaling.

Oh no. That's not good. We get a `ValueError` output. 

You see, `scikit-learn` only accepts numeric data as an input and it's not sure how to handle the `ocean_proximity` feature.
:::

---

<br>

### So what do we do? 

- Drop the column (not recommended)
- We can transform categorical features to numeric ones so that we can use them in the model
- There are two transformations we can do:
    - <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" target="_blank">Ordinal encoding</a>
    - <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank">One-hot encoding</a>(recommended in most cases)


:::{.notes} 
We could drop the column as we did in the previous module and we get descend enough scores, but this won't always be the case. 

Just like we said, about dropping a column due to missing values, we don't want to throw away information that could be useful for helping our model and prediction. 

Or we can give `scikit-learn` what it wants! We can transform our categorical features into numeric ones so we can use them in our models. 


There are 2 types of ways we are going to talk about doing this:

-  <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" target="_blank">Ordinal encoding</a> (occasionally recommended)
- One-hot encoding (OHE -recommended in most cases)
:::

---

## Ordinal encoding

```{python}
# | include: false
X_toy = pd.DataFrame({'rating':['Good', 'Bad', 'Good', 'Good', 
                                  'Bad', 'Neutral', 'Good', 'Good', 
                                  'Neutral', 'Neutral', 'Neutral','Good', 
                                  'Bad', 'Good']})
```

```{python}
X_toy
```

```{python}
pd.DataFrame(X_toy['rating'].value_counts()).rename(columns={'rating': 'frequency'}).T
```


:::{.notes} 
Let's take a look at a dummy dataframe to explain how to use ordinal encoding. 

Here we have a categorical column specifying different movie rating.
:::

---

```{python}
from sklearn.preprocessing import OrdinalEncoder

oe = OrdinalEncoder(dtype=int)
oe.fit(X_toy);
X_toy_ord = oe.transform(X_toy)

X_toy_ord
```


:::{.notes} 
Here we simply assign an integer to each of our unique categorical labels.

We can use sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" target="_blank">`OrdinalEncoder`</a>.

First, we import `OrdinalEncoder` from `sklearn.preprocessing`.

`OrdinalEncoder` is a transformer just like `SimpleImputer` and `StandardScaler` so we initial our encoder and then we fit and transform, just like we did with numeric columns. 
:::

---

```{python}
encoding_view = X_toy.assign(rating_enc=X_toy_ord)
encoding_view
```


:::{.notes} 
Since `sklearn`'s transformed output is an array, we can add it next to our original column to see what happened. 

In this case, we can see that each rating has been designated an integer value. 

For example, `Neutral` is represented by an encoded value of 2 and `Good` a value of 1. Shouldn't `Good` have a higher value? 
:::

---

```{python}
ratings_order = ['Bad', 'Neutral', 'Good']
```

<br>

```{python}
oe = OrdinalEncoder(categories = [ratings_order], dtype=int)
oe.fit(X_toy);
X_toy_ord = oe.transform(X_toy)

X_toy_ord
```


:::{.notes}
We can change that by specifying the order in the `categories` argument.
:::

---

```{python}
encoding_view = X_toy.assign(rating_enc=X_toy_ord)
encoding_view
```

:::{.notes}
Now our `Good` rating is given an ordinal value of 2 and the `Bad` rating is encoded as 0.
:::

---

```{python}
# | include: false
X_toy = pd.DataFrame({'language':['English', 'Vietnamese', 'English', 'Mandarin', 
                                  'English', 'English', 'Mandarin', 'English', 
                                  'Vietnamese', 'Mandarin', 'French','Spanish', 
                                  'Mandarin', 'Hindi']})
```

```{python}
X_toy
```

```{python}
pd.DataFrame(X_toy['language'].value_counts()).rename(columns={'language': 'frequency'}).T
```


:::{.notes} 
Let's try this again but with now we have a categorical column specifying different languages. 
:::

---

```{python}
from sklearn.preprocessing import OrdinalEncoder

oe = OrdinalEncoder(dtype=int)
oe.fit(X_toy);
X_toy_ord = oe.transform(X_toy)

encoding_view = X_toy.assign(language_enc=X_toy_ord)
encoding_view
```


:::{.notes} 
We assign an integer to each of our unique categorical labels and now all our languages are encoded. 

Does it make sense to have this column ordinally encoded though? 

We are saying in this case that `Vietnamese` is  as close to `spanish` as `Mandarin` is to `Hindi`. Does that quantification make sense? We can't really compare these things and that is why we use One-hot encoding which is what we will look at in the next section. 
:::


# Let‚Äôs apply what we learned!
