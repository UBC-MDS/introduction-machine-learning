---
format: revealjs
title: Decision Tree Regressor
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```


```{python}
regression_df = pd.read_csv("data/quiz2-grade-toy-regression.csv")
regression_df
```


:::{.notes}
We saw previously that we can use decision trees for classification problems but we can also use this decision tree algorithm for regression problems.  

Instead of using Gini (which we briefly mentioned this in previous slides), we can use <a href="https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation" target="_blank">some other criteria</a> for splitting. 
(A common one is mean squared error (MSE) which we will discuss shortly)

`scikit-learn` supports regression using decision trees with `DecisionTreeRegressor()` and the `.fit()` and `.predict()` paradigm that is similar to classification.

Just like when we talked about the baseline Dummy regressor, `.score()` for regression returns somethings called an <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" target="_blank"> ùëÖ2 </a>.    

The maximum ùëÖ2 is 1 for perfect predictions. 

It can be negative which is very bad (worse than DummyRegressor). 
:::

--- 

```{python}
X = regression_df.drop(columns=["quiz2"])
X.head()
```

<br>

```{python}
y = regression_df["quiz2"]
y.head()
```

:::{.notes}
Before we do anything, let's bring in our regression data this time it's going to be the quiz2 dataset however instead of predicting quiz2 a categorical variable (A+ or Not A+) we have continuous values instead.

We split our data into our `X` and `Y` objects as we‚Äôve previously been doing.
:::

---

```{python}
# | output: false
from sklearn.tree import DecisionTreeRegressor

depth = 4
reg_model = DecisionTreeRegressor(max_depth=depth)
reg_model.fit(X, y)
```


:::{.notes}
**Decision Tree Regressor** is built using `DecisionTreeRegressor()` and a similar `.fit()` and `.predict()` paradigms as classification.

Instead of importing `DecisionTreeClassifier`, we import `DecisionTreeRegressor`.

We follow the same steps as before and can even set hyperparameters as we did in classification. 

Here when we build our model, we are specifying a `max_depth` of 4. 

This means our decision tree is going to be constrained to a depth of 4.
:::

---

```{python}
# | include: false
# | eval: false
import graphviz
from sklearn.tree import export_graphviz

dot_data = export_graphviz(model)
graphviz.Source(
    export_graphviz(
        model4,
        out_file=None,
        feature_names=X.columns,
        class_names=["red", "blue"],
        impurity=True,
    )   
).render("../../../../static/module2/module2_16a", format='png') 
```

```{python}
# | include: false
# | eval: false
from src.display_tree import display_tree

display_tree(X.columns, reg_model, "../../../../static/module2/module2_16a")
```

![](../../../static/module2/module2_16a.png){fig-align="center" width="68%" fig-alt="404 image"}

    
:::{.notes}
And here is the tree it produces.

We can see all the decision boundaries and splitting values. 

Our leaves used to contain a categorical value for prediction, but this time we see our leaves are predicting numerical values.
:::

---

```{python}
X.loc[[0]]
```

<br>

```{python}
reg_model.predict(X.loc[[0]])
```

:::{.notes}
Here we take a single example. 

This example has `class_attendance` and `ml_experience` equal to 1 and then the numerical values for labs 1-4 and `quiz2`. 

When we predict on this single example, we can see that our model outputs a value of 90. 
:::

---

```{python}
predicted_grades = reg_model.predict(X)
regression_df = regression_df.assign(predicted_quiz2 = predicted_grades)
print("R^2 score on the training data:" + str(round(reg_model.score(X,y), 3)))
```

<br>

```{python}
regression_df.head()
```



:::{.notes}
Let's see how well this model does predicting on the entire data.

Now we are using `.score()` on the entire data that we‚Äôve trained on. 

We can compare the predicted value versus the true quiz2 grade in this dataframe and we see our model has predicted every example correctly. 

This is confirmed when we see that the score is 1.0. 

We talked in Module 1 about how we use a measurement called ùëÖ2 to measure the score of regression models. An ùëÖ2 score of 1.0, means the model perfectly predicts the outcome of every observation.

This is quite different from what we were getting with a Dummy Classifier which had an ùëÖ2 value of 0.
:::


# Let‚Äôs apply what we learned!
