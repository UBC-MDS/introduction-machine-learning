---
format: revealjs
title: ùëò -Nearest Neighbours Regressor
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```

## Regression with ùëò-nearest neighbours ( ùëò -NNs)

```{python}
np.random.seed(0)
n = 50
X_1 = np.linspace(0,2,n)+np.random.randn(n)*0.01
X = pd.DataFrame(X_1[:,None], columns=['length'])
X.head()
```

<br>

```{python}
y = abs(np.random.randn(n,1))*2 + X_1[:,None]*5
y = pd.DataFrame(y, columns=['weight'])
y.head()
```



:::{.notes} 
We can use the ùëò-nearest neighbour algorithm on regression problems as well.

 In ùëò-nearest neighbour regression, we take the average of ùëò-nearest neighbours instead of majority vote.

 Let's look at an example. Here we are creating some synthetic data with fifty examples and only one feature. 

Let's imagine that our one feature represents the length of a snake and our task is to predict the weight of the snake given the length. 

Right now, do not worry about the code and only focus on data and our model. 
:::

---

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
```

<br>
```{python}
# | output: false
import altair as alt
source = pd.concat([X_train, y_train], axis=1)

scatter = alt.Chart(source, width=500, height=300).mark_point(filled=True, color='green').encode(
    alt.X('length:Q'),
    alt.Y('weight:Q'))

scatter
```

```{python}
# | include: false
scatter.save('static/module4/snakes2.png')
```
![](../../../static/module4/snakes2.png){fig-alt="404 image" width="65%" fig-align="center"}


:::{.notes}
Let's split over data first so there we do not break the golden rule of machine learning.

And here is what our data looks like.

We only have one feature of `length` and our goal is to predict `weight`. 
:::

---

```{python}
from sklearn.neighbors import KNeighborsRegressor

knnr = KNeighborsRegressor(n_neighbors=1, weights="uniform")
knnr.fit(X_train,y_train);
```

<br>

```{python}
predicted = knnr.predict(X_train)
predicted[:5]
```

<br>

```{python}
knnr.score( X_train, y_train)  
```



:::{.notes} 
Now let's try the ùëò-nearest neighbours regressor on this data. 

In this case, we import `KNeighborsRegressor` instead of `KNeighborsClassifier`.  

Then we create our `KNeighborsRegressor` object with `n_neighbors=1` so we are only considering 1 neighbour  and with `uniform` weights. 

We fit our model and predict on `X_train`. 

Here are the first five predictions. 

As expected we get continuous values as predictions. 

If we scored over regressors we get this perfect score of one. 

Now remember that we are using a `n_neighbors=1`, so we are likely to overfit.
:::

---

```{python}
# | include: false
n = 50  # number of samples
np.random.seed(0)  # fix seed for reproducibility
X = np.linspace(0,2,n)+np.random.randn(n)*0.01
X = X[:, None]
y = abs(np.random.randn(n,1))*2 + X_1[:,None]*5

knn = KNeighborsRegressor(n_neighbors=1, weights="uniform")
knn.fit(X, y)
```

```{python}
# | echo: false
# | fig-align: center
import matplotlib.pyplot as plt
plt.figure(figsize=(8,4))
grid = np.linspace(np.min(X), np.max(X), 1000)[:, None]
plt.plot(grid, knn.predict(grid), color='orange', linewidth=1)
plt.plot(X, y, ".r", markersize=10, color='green')
plt.xticks(fontsize= 14);
plt.yticks(fontsize= 14);
plt.xlabel("length",fontsize= 14)
plt.ylabel("weight",fontsize= 14);
```



:::{.notes} 
Here is how our model would look like if we plotted it. 

The model is trying to get every example correct since `n_neighbors=1`.
:::

---

```{python}
knnr = KNeighborsRegressor(n_neighbors=10, weights="uniform")
knnr.fit(X_train, y_train);
```

<br>

```{python}
knnr.score(X_train, y_train)
```

<br>

```{python}
# | echo: false
# | fig-align: center
knn = KNeighborsRegressor(n_neighbors=10, weights="uniform")
knn.fit(X, y);
plt.figure(figsize=(8,4))
plt.plot(grid, knn.predict(grid), color='orange', linewidth=1)
plt.plot(X, y, ".r", markersize=10, color='green')
plt.xticks(fontsize= 16);
plt.yticks(fontsize= 16);
plt.xlabel("length",fontsize= 16)
plt.ylabel("weight",fontsize= 16);
```

:::{.notes} 
 Now let's try `n_neighbors=10`.

Again, we are creating our `KNeighborsRegressor` object with `n_neighbors=10` and ` `n_neighbors=10`=‚Äôuniform‚Äô` which means all of our examples have equal contribution to the prediction.

We fit our regressor and score it. Now we can see we are getting a lower score over the training set. Our score decreased from 1.0 when to had `n_neighbors=1` to now having a score of 0.932.  

When we plot our model, we can see that it no longer is trying to get every example correct.
:::

---

## Using weighted distances

```{python}
knnr = KNeighborsRegressor(n_neighbors=10, weights="distance")
knnr.fit(X_train, y_train);
```

<br>

```{python}
knnr.score(X_train, y_train)
```

```{python}
# | echo: false
# | fig-align: center
plt.figure(figsize=(8,4))
knn = KNeighborsRegressor(n_neighbors=10, weights="distance").fit(X, y);
plt.plot(grid, knn.predict(grid), color='orange', linewidth=1)
plt.plot(X, y, ".r", markersize=10, color='green')
plt.xticks(fontsize= 16);
plt.yticks(fontsize= 16);
plt.xlabel("length",fontsize= 16)
plt.ylabel("weight",fontsize= 16);
```


:::{.notes} 
Let's now take a look at the `weight` hyperparameter `distance`. 

This means that the points (examples) that are closer now have more meaning to the prediction than the points (example) that are further away. 

If we use this parameter, fit it and then score it, we get a perfect training score again. 

Plotting it shows that the model is trying to predict every model correctly. This is likely another situation of overfitting. 
:::

---

## Pros and Cons of ùëò -Nearest Neighbours

<br>

### Pros:

- Easy to understand, interpret.
- Simply hyperparameter ùëò (`n_neighbors`) controlling the fundamental tradeoff.
- Can learn very complex functions given enough data.
- Lazy learning: Takes no time to `fit`

<br>

### Cons:

- Can potentially be VERY slow during prediction time. 
- Often not that great test accuracy compared to the modern approaches.
- You should scale your features. We'll be looking into it in the next lecture. 

:::{.notes} 
Let's talk about some pros and cons.

Advantages include:

- Easy to understand and interpret.
- Simply hyperparameter ùëò (`n_neighbors`) controlling the fundamental trade-off.
    - lower ùëò is likely producing an overfit model and higher ùëò is likely producing an underfit model. 
- Given the simplicity of this algorithm, it can surprisingly learn very complex functions given enough data. 
- ùëò-Nearest Neighbours we don't really do anything during the fit phase. 

Some disadvantages often include:

- Can potentially be quite slow during prediction time which is due to the fact that it does very little during training time. During prediction, the model must find the distances to the query point to all examples in the training set and this makes it very slow.
- Scaling must be done when using this model, which will be covered in module 5. 
:::


# Let‚Äôs apply what we learned!
