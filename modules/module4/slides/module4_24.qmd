---
format: revealjs
title: Support Vector Machines (SVMs) with RBF Kernel
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```


```{python}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)
train_df.head()
```

<br>

```{python}
X_train, y_train = train_df.drop(columns=['country']), train_df['country']
X_test, y_test = test_df.drop(columns=['country']), test_df['country']
X_train.head()
```

:::{.notes} 
Another popular similarity-based algorithm is Support Vector Machines (SVM).

SVMs use a different similarity metric which is called a "kernel" in SVM land. 

We are going to concentrate on the specific kernel called Radial Basis Functions (RBFs).

Let's bring back our trusty cities dataset again. 
:::

---


```{python}
# | output: false
cities_plot = alt.Chart(train_df).mark_circle(size=20, opacity=0.6).encode(
    alt.X('longitude:Q', scale=alt.Scale(domain=[-140, -40])),
    alt.Y('latitude:Q', scale=alt.Scale(domain=[20, 60])),
    alt.Color('country:N', scale=alt.Scale(domain=['Canada', 'USA'],
                                           range=['red', 'blue'])))
cities_plot
```

```{python}
# | include: false
cities_plot.save('static/module4/cities_plot.png')
```

![](../../../static/module4/cities_plot.png){fig-alt="404 image" width="60%"}



:::{.notes} 
Here is our data plotted once again with the red dots representing Canadian cities and the blue ones represent American cities. 
:::

---

```{python}
from sklearn.svm import SVC
from sklearn.model_selection import cross_validate

svm = SVC(gamma=0.01)
scores = cross_validate(svm, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```

<br>

```{python}
svm_cv_score = scores['test_score'].mean()
svm_cv_score
```


:::{.notes} 
In this course, we are not going into detail about how support vector machine classifiers or regressor works but more so on how to use it with Scikit-learn. 

We must first import the necessary library and then we can get to model building in the same fashion as we did before.  

Here we are importing the `SVC` tool from the `sklearn.svm` library. 

For now, just ignore the gamma input argument in `SVC()` we will get to that soon. 

After building our model and performing cross-validation, we can see that the mean accuracy is 0.820. 
:::

---

```{python}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)
X_train, y_train = train_df.drop(columns=['country']), train_df['country']
X_test, y_test = test_df.drop(columns=['country']), test_df['country']
```


```{python}
# | include: false
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train);
scores = cross_validate(knn, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```


```{python}
# | echo: false
import matplotlib.pyplot as plt
from src.model_plotting_mg import plot_classifier

y_encoded_train = np.where(y_train == 'Canada', 0, 1)
svm.fit(X_train, y_encoded_train);
knn.fit(X_train, y_encoded_train);
plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plt.title("SVC")
plot_classifier(X_train.to_numpy(), y_encoded_train, svm, ax=plt.gca())
plt.subplot(1, 2, 2)
plt.title("KNN with k = 5")
plot_classifier(X_train.to_numpy(), y_encoded_train, knn, ax=plt.gca())
```


:::{.notes} 
If we plot over the support vector machine classifier along with the ùëò-Nearest Neighbours classifier, we can see that the support vector machine classifier is a smoothed version of the ùëò-Nearest Neighbours classifier. 
:::

---

## SVMs

```{python}
knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_validate(knn, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```

<br>

```{python}
knn_cv_score = scores['test_score'].mean().round(3)
knn_cv_score
```

<br>

```{python}
svm_cv_score.round(3)
```

:::{.notes} 
Superficially, support vector machines are very similar to ùëò-Nearest Neighbours.

A test example is positive if on average it looks more like positive examples it is negative if on average it looks more like negative examples.

The primary difference between ùëò-NNs and SVMs is that:

- Unlike ùëò-NNs, SVMs only remember the key examples (support vectors). 
- When it comes to predicting a query point, we only consider the key examples from the data, and only calculate the distance to these key examples. This makes it more efficient than ùëò-NN. 

If we compare the scores from the ùëò-NN model using `n_neighbors=5` and the scores from the SVM model we get similar results, however, the SVM model seems to do slightly better than the ùëò-NN model. 
:::

---

## SVM Regressor 


```{python}
from sklearn.svm import SVR
```


:::{.notes}
It should come as no surprise that we can use SVM models for regression problems as well.

We need to make sure to import SVR from the SVM sklearn library. 
:::

---

## Hyperparameters of SVM are:

- `gamma`
- `C`
    


:::{.notes} 
There are 2 main hyperparameters for support vector machines with an RBF kernel; `gamma` and `C`.

We are not going into detail about the interpretation of these hyperparameters but we will observe how they are related to the fundamental trade-off. 

If you wish to learn more on these you can reference <a href="https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html" target="_blank">Scikit-learn`'s explanation of RBF SVM parameters</a>.
:::

---

### Relation of gamma and the fundamental trade-off

`gamma` controls the complexity of a model, just like other hyperparameters we've seen.

- As  gamma  ‚Üë, complexity  ‚Üë
- As  gamma  ‚Üì, complexity ‚Üì

<br>

```{python}
# | echo: false
y_encoded_train = np.where(y_train == 'Canada', 0, 1)

plt.figure(figsize=(16, 4))
for i in range(4):
    plt.subplot(1, 4, i + 1)
    gamma = 10.0 ** (i - 3)
    rbf_svm = SVC(gamma=gamma)
    rbf_svm.fit(X_train, y_encoded_train);
    plt.title("gamma = %s" % gamma);
    plot_classifier(X_train.to_numpy(), y_encoded_train, rbf_svm, ax=plt.gca(), show_data=False);
```


:::{.notes} 
The first type of hyperparameter is `gamma`. `gamma` controls the complexity of the model.

Using higher values for `gamma` means a more complex model is produced whereas lower values result in a less complex model. 

If we look at the plots, it appears that with lower values of gamma, the model is likely underfitting, and as gamma increases, the potential of overfitting is also increasing.
:::

---

## Relation of C and the fundamental trade-off

C also affects the fundamental tradeoff.

- As  C  ‚Üë, complexity  ‚Üë
- As  C  ‚Üì, complexity ‚Üì

<br>

```{python }
# | echo: false
y_encoded_train = np.where(y_train == 'Canada', 0, 1)

plt.figure(figsize=(16, 4))
for i in range(4):
    plt.subplot(1, 4, i + 1)
    C = 10.0 ** (i - 1)
    rbf_svm = SVC(C=C, gamma=0.01)
    rbf_svm.fit(X_train, y_encoded_train);
    plt.title("C = %s" % C);
    plot_classifier(X_train.to_numpy(), y_encoded_train, rbf_svm, ax=plt.gca(), show_data=False);
```


:::{.notes} 
The other hyperparameter we will look at is `C`. `C` also controls the fundamental trade-off. Just like with gamma, higher values increase the model complexity whereas lower values decrease the complexity. 

Obtaining optimal validation scores requires a hyperparameter search between both `gamma` and `C` to balance the fundamental trade-off.

We will learn how to search over multiple hyperparameters at a time in the next module. 
:::


# Let‚Äôs apply what we learned!