---
format: revealjs
title: Choosing ùëò (n_neighbors)
title-slide-attributes:
  data-notes: |
    We saw that the prediction of over query pointing changes with different values for the `n_neighbors` argument.

    So, a natural question is _how do we pick `n_neighbors`?_. 

    What happens when we change this hyperparameter? 

    Are we likely to be overfitting or underfitting with higher or lower values of `n_neighbors`?
---

```{python}
#  | echo: false
%run src/utils.py
```


```{python}
from sklearn.model_selection import train_test_split

cities_df = pd.read_csv("data/canada_usa_cities.csv")
X = cities_df.drop(columns=["country"])
y = cities_df["country"]
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=123)
```

<br>

```{python}
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=1)
model.fit(X_train, y_train.to_numpy());
```

<br>

```{python}
model.score(X_train,y_train)
```

:::{.notes} 
Let's examine this using our cities data. 

As usual, we create our `X` and `y` objects as well as our training and test splits.

We create our `KNeighborsClassifier` object with `n_neighbors=1` and training the model. 

When we score it, we get an accuracy of 1 on the training data. 
:::

---

```{python}
from sklearn.model_selection import cross_validate

k = 1
knn1 = KNeighborsClassifier(n_neighbors=k)
scores = cross_validate(knn1, X_train, y_train, return_train_score = True)
pd.DataFrame(scores)
```

<br>

```{python}
k = 100
knn100 = KNeighborsClassifier(n_neighbors=k)
scores = cross_validate(knn100, X_train, y_train, return_train_score = True)
pd.DataFrame(scores)
```

:::{.notes} 
Let's carry out cross-validation with ùëò=1. 

These are our cross-validation results. 

What we see here is in each fold our training score always produces a perfect accuracy of 1.0.

Our validation score for each fold is much lower than the training score. 

The gap between the training and validation sets seems to be high and so it is likely that our model is overfitting. 

Let‚Äôs seen now what happens when ùëò=100. 

Now, we see that our training scores are much lower and our validation scores are also lower. 

The gap between the training and validation sets seem to be lower. This looks like our model is underfitting now. 
:::

---

```{python}
# | echo: false
import matplotlib.pyplot as plt
from src.model_plotting_mg import plot_classifier

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)

y_encoded_train = np.where(y_train == 'Canada', 1, 0)

knn1.fit(X_train, y_encoded_train);
plt.title("n_neighbors = 1")
plt.ylabel("latitude")
plt.xlabel("longitude")
plot_classifier(X_train.to_numpy(), y_encoded_train, knn1, ax=plt.gca(), ticks=True)

plt.subplot(1, 2, 2)
plt.title("n_neighbors = 100")
knn100.fit(X_train, y_encoded_train);
plt.ylabel("latitude")
plt.xlabel("longitude")
plot_classifier(X_train.to_numpy(), y_encoded_train, knn100, ax=plt.gca(), ticks=True)
```

:::{.notes} 
If we plot these two models with ùëò=1 on the left and ùëò=100 on the right. 

The left plot shows a much more complex model where it is much more specific and attempts to get every example correct. 

The plot on right is plotting a simpler model and we can see more training examples are being predicted incorrectly. 
:::

---

## How to choose `n_neighbors`?

```{python}
results_dict = {"n_neighbors": list(), "mean_train_score": list(), "mean_cv_score": list()}

for k in range(1,50,5):
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_validate(knn, X_train, y_train, return_train_score = True)
    results_dict["n_neighbors"].append(k)
    results_dict["mean_cv_score"].append(np.mean(scores["test_score"]))
    results_dict["mean_train_score"].append(np.mean(scores["train_score"]))

results_df = pd.DataFrame(results_dict)
results_df
```



:::{.notes} 
In our toy problem with ùëò=1, we saw the model was overfitting yet when ùëò=100, the model was underfitting. 

So, the question is how do we pick ùëò?

- The answer lies in hyperparameter optimization. 

Here we are looping over different values of ùëò ( `n_neighbors`) and performing cross-validation on each one. 
:::

---

```{python}
# | include: false
plotting_source = results_df.melt(id_vars='n_neighbors', 
                                  value_vars=['mean_train_score', 'mean_cv_score'], 
                                  var_name='score_type' ,
                                  value_name= 'accuracy' )
                                  
                                  
K_plot = alt.Chart(plotting_source, width=500, height=300).mark_line().encode(
    alt.X('n_neighbors:Q'),
    alt.Y('accuracy:Q', scale=alt.Scale(domain=[.67, 1.00])),
    alt.Color('score_type:N')
).properties(title="Accuracies of n_neighbors for KNeighborsClassifier")


```

```{python}
# | include: false
K_plot.save('static/module4/K_plot.png')
```

<br>
![](../../../static/module4/K_plot.png){fig-align="center" fig-alt="404 image" width="80%"}


:::{.notes} 
In this graph we‚Äôve plotted, `n_neighbors` is on the x-axis and the model accuracy is on the y-axis.

We can see there is a sweet spot where the gap between the validation and training scores is the lowest. Here it‚Äôs when `n_neighbors` is 11.
:::

---


```{python}
sorted_results_df = results_df.sort_values("mean_cv_score", ascending = False)
sorted_results_df
```

<br>

```{python}
best_k = sorted_results_df.iloc[0,0]
best_k
```

:::{.notes} 
We can find the most optimal `n_neighbors` value by sorting our results on the mean validation score. 

This shows the best validation score occurs when `n_neighbors=11`.
:::

---

```{python}
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train);
print("Test accuracy:", round(knn.score(X_test, y_test), 3))
```


:::{.notes} 
Now that we know the best scoring hyperparameter value, we are ready to assess our model on the test set. 

We recreate our model with `n_neighbors=11`, fit the model and score it on the test set. 

Our testing accuracy is 0.905 which is higher than the validation mean accuracy we had earlier. 

This is surprising and could be due to having a small dataset.
:::

---

## Curse of dimensionality


<br> 

- ùëò -NN usually works well when the number of dimensions is small.

<br>
<br>


![](../../../static/module4/skull.png){fig-align="center" fig-alt="404 image" width="60%"}



:::{.notes}
In the previous module, we discussed one of the most important problems in machine learning which was overfitting the second most important problem in machine learning is the ***curse of dimensionality***.

This problem affects most models but this problem is especially bad for ùëò-NN. 

ùëò-NN works well then the number of dimensions is small but things fall apart fairly quickly as the number of dimensions goes up. 

If there are many irrelevant features, ùëò-NN is hopelessly confused because all of them contribute to finding similarities between examples. 

With enough irrelevant features, the accidental similarity between features wipe out any meaningful similarity and ùëò-NN becomes is no better than random guessing.
:::

---

## Other useful arguments of `KNeighborsClassifier`


![](../../../static/module4/knn.png){fig-align="center" fig-alt="404 image" width="90%"}

<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" target="_blank">**Attribution** </a>  



:::{.notes} 
Another useful hyperparameter is `weight`. 

So far, when predicting labels, we have been giving equal weight to all the nearby examples. 

We can change that using this `weight` hyperparameter. 

We can tell it to weigh the examples higher if they are closer to the query point. 
:::


# Let‚Äôs apply what we learned!