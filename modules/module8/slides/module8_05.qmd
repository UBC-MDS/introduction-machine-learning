---
format: revealjs
title: Coefficients and coef_
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```

## Intuition behind linear regression


| listing number | Number of Bedrooms | Number of Bathrooms | Square Footage | Age | Price |
|----------------|--------------------|---------------------|----------------|-----|-------| 
| 1              | 5                  | 6                   | 3000           | 2   | $6.39 million|
| 2              | 1                  | 1                   | 800            | 90  | $1.67 million|
| 3              | 3                  | 2                   | 1875           | 66  | $3.92 million|
: {tbl-colwidths="[16,16,16,16,10,16]"}

:::{.notes}
Unlike with decision trees where we make predictions with rules and analogy-based models where we predict a certain class using distance to other examples, linear classifiers use **coefficients** (or sometimes known as "weights") associated with features.
:::

---

![](../../../static/module8/house_table.png){fig-align="center" width="50%" fig-alt="404 image"}


:::{.notes}
We then use these learned coefficients to make predictions.

For example, suppose we are predicting the price of a house and we have 4 features;  number of bedrooms, number of bathrooms, square footage, and age.
:::

---

![](../../../static/module8/house_table.png){fig-align="center" width="35%" fig-alt="404 image"}

<br>

Consider the following listing (example):     

| listing number | Number of Bedrooms | Number of Bathrooms | Square Footage | Age |
|----------------|--------------------|---------------------|----------------|-----| 
| 3              | 3                  | 2                   | 1875           | 66  |


&nbsp;&nbsp;

<font size="4"><em> predicted(price) = coefficient<sub>bedrooms</sub> x #bedrooms + coefficient<sub>bathrooms</sub> x #bathrooms + coefficient<sub>sqfeet</sub> x #sqfeet + coefficient<sub>age</sub> x age + intercept </em></font>


<font size="4"><em> predicted(price) = 0.03 x #bedrooms + 0.04 x #bathrooms + 0.002 x #sqfeet + -0.01 x #age + intercept </em></font>


<font size="4"><em> predicted(price) = (0.03 x 3) + (0.04 x 2) + (0.002 x 1875) + (-0.01 x 66) + 0 </em></font>

<font size="4"><em> predicted(price) = 3.26 </em></font>


---

## Components of a linear model

<br>
<br>
<br>

<font size="5"><em> predicted(price) = (<font  color="#b1d78c">coefficient<sub>bedrooms</sub></font>   x  <font  color="7bd1ec">#bedrooms</font>)   +  (<font  color="#b1d78c">coefficient<sub>bathrooms</sub></font>  x  <font  color="7bd1ec">#bathrooms</font>)  +  (<font  color="#b1d78c">coefficient<sub>sqfeet</sub></font>   x   <font  color="7bd1ec">#sqfeet</font>)  +  (<font  color="#b1d78c">coefficient<sub>age</sub></font>  x <font  color="7bd1ec">age</font>)  +  <font  color="e8b0d0">intercept</font> </em> </font> 

- <font  color="7bd1ec"> Input features</font>     
- <font  color="#b1d78c"> Coefficients, one per feature</font>      
- <font  color="e8b0d0"> Bias or intercept</font> 




---


```{python}
housing_df = pd.read_csv("data/real_estate.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=1)
train_df.head()
```

<br>

```{python}
X_train, y_train = train_df.drop(columns =['price']), train_df['price']
X_test, y_test = test_df.drop(columns =['price']), test_df['price']
```


:::{.notes} 
Let's now use `Ridge` with our Taiwan housing dataset again from the last slide deck. Here, we want to predict the house price. 
:::

---

```{python}
from sklearn.linear_model import Ridge

lm = Ridge()
lm.fit(X_train, y_train);
training_score = lm.score(X_train, y_train)
training_score
```

<br>

```{python}
lm.coef_
```

:::{.notes} 
We can make our model as usual and train it, and assess our training score.

We saw that with linear classifiers we have coefficients associated with each feature of our model. 

How do we get that? We can use `.coef_` to obtain them from our trained model. 

But how are these useful?
:::

---

```{python}
ridge_coeffs = lm.coef_
ridge_coeffs
```

<br>

```{python}
words_coeffs_df = pd.DataFrame(data=ridge_coeffs, index=X_train.columns, columns=['Coefficients'])
words_coeffs_df
``` 


:::{.notes} 
One of the primary advantages of linear classifiers is their ability to interpret models using these coefficients. 

What do these mean? Let's try to make some sense of it here. 

We have our coefficients but we should see which feature corresponds to which coefficient. 

We can do that by making a dataframe with both values. 

We can use these coefficients to interpret our model. They show us how much each of these features affects our model's prediction.

For example, if we had a house with 2 stores nearby, our `num_stores` value is 2. That means that 2 * 1.26 = 2.52 will contribute to our predicted price!

The negative coefficients work in the opposite way, for example, every unit increase in age of a house will, subtracts 0.244 from the house's predicted value.
:::

---

```{python}
words_coeffs_df.abs().sort_values(by='Coefficients')
```

:::{.notes}
In linear models, the coefficients tell us how each feature affects the prediction. 

So, looking at the features which have coefficient with bigger magnitudes might be useful and contribute more to the prediction. 

It's important to be careful here though because this depends on the scaling of the features. Larger features will have smaller coefficients, but if we scale our features before we build our model then they are on a somewhat level playing field! (Another reason we should be scaling our features!)
:::

---

## Interpreting learned coefficients

<br>

In linear models: 

- if the coefficient is +, then ↑ the feature values ↑ the prediction value.  
- if the coefficient is -, then ↑ the feature values ↓ the prediction value.  
- if the coefficient is 0,  the feature is not used in making a prediction.



:::{.notes}
In linear models: 

- if the coefficient is positive, then increasing the feature values increases the prediction value.  
- if the coefficient is negative, then increasing the feature values decreases the prediction value.  
- if the coefficient is zero, the feature is not used in making a prediction
:::

---

## Predicting


```{python}
X_train.iloc[0:1]
```

<br>

```{python}
lm.predict(X_train.iloc[0:1])
```



:::{.notes} 
Let's take a look at a single example here. 

The values in this are the input features. 

We can use `predict()` on our features to get a prediction of 52.36.
:::

---

```{python}
words_coeffs_df.T
```

<br>

```{python}
X_train.iloc[0:1]
```

<br>

```{python}
intercept = lm.intercept_
intercept
```



:::{.notes} 
Using our coefficients, and the model's intercept we can calculate the model's predictions ourselves as well.
:::

---

<center><font size="3"><em> predicted(price) = coefficient<sub>house_age</sub> x house_age + coefficient<sub>distance_station</sub> x distance_station + coefficient<sub>num_stores</sub> x num_stores + coefficient<sub>latitude</sub> x latitude +  coefficient<sub>longitude</sub> x longitude + intercept </em></font></center>

<br>

```{python}
(ridge_coeffs * X_train.iloc[0:1]).sum(axis=1) + intercept 
```

<br>

```{python}
lm.predict(X_train.iloc[0:1])
```

:::{.notes}
All of these feature values multiplied by the coefficients then adding the intercept, contribute to our prediction. 

When we do this by hand using the model's coefficients and intercept, we get the same as if we used `predict`.
:::


# Let’s apply what we learned!