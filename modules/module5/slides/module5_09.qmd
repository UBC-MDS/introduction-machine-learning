---
format: revealjs
title: "Case Study: Preprocessing with Scaling"
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```

```python
knn_unscaled = KNeighborsClassifier();
knn_unscaled.fit(X_train, y_train)
print('Train score: ', (round(knn_unscaled.score(X_train, y_train), 2)))
print('Test score: ', (round(knn_unscaled.score(X_test, y_test), 2)))
```
 
```out
KNeighborsClassifier()
Train score:  0.71
Test score:  0.45
```

<br>
 
```python
knn_scaled = KNeighborsClassifier();
knn_scaled.fit(X_train_scaled, y_train)
print('Train score: ', (round(knn_scaled.score(X_train_scaled, y_train), 2)))
print('Test score: ', (round(knn_scaled.score(X_test_scaled, y_test), 2)))
```
 
```out
KNeighborsClassifier()
Train score:  0.94
Test score:  0.89
```
 
 
:::{.notes}
We've seen why scaling is important when we were using our basketball dataset in the first section of this module.
 
In this section, we are going to dive a little deeper into the process and the transformer options. 

We had our basketball dataset and we saw the training and testing scores before scaling with ùëò-NN and we saw how these results improved when we scaled our features.
:::

---
 
## Scaling
 
 
![](../../../static/module5/scaling-data.png){fig-align="center" width="80%" fig-alt="404 image"}

<a href="https://amueller.github.io/COMS4995-s19/slides/aml-05-preprocessing/#8" target="_blank">Attribution</a>
 
 
:::{.notes}
This problem affects a large number of ML methods.
 
There are several approaches to this problem.

This diagram shows the original data with each feature on an axis and the 4 diagrams on the right show how the data is transformed with different scaling methods. 
 
We are going to concentrate on the two named `MinMaxScaler` and `StandardScaler`.
:::

---

| Approach | What it does | How to update ùëã (but see below!) | sklearn implementation |
|---------|------------|-----------------------|----------------|
| Normalization | sets range to [0,1]   | `X -= np.min(X, axis=0)`<br>`X /= np.max(X, axis=0)` | <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" target="_blank">`MinMaxScaler()`</a> |
| Standardization | sets sample mean to 0, s.d. to 1   | `X -= np.mean(X, axis=0)`<br>`X /=  np.std(X, axis=0)` | <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" target="_blank">`StandardScaler()`</a>|  

&nbsp;

There are all sorts of articles on this: see
<a href="http://www.dataminingblog.com/standardization-vs-normalization/" target="_blank">here</a>
and <a href="https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc" target="_blank">here</a>.
 
:::{.notes}
In terms of terminology, **Normalization** is the term we use when we use the `MinMaxScaler()` function and **Standardization** with `StandardScaler()` which we saw before. 

Normalization converts all the values so they lie between the values 0 and 1. 
Standardization will set the sample mean to 0 and distribute the values around the mean with a standard deviation of 1. 
:::

---
 
```{python}
# | include: false
from sklearn.impute import SimpleImputer

housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)
 
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 
 
X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]
 
X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
 
imputer = SimpleImputer(strategy="median")
imputer.fit(X_train)
X_train_imp = imputer.transform(X_train)
X_test_imp = imputer.transform(X_test)
```

```{python}
pd.DataFrame(X_train_imp, columns=X_train.columns, index=X_train.index).head()
```

<br>

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imp)
X_test_scaled = scaler.transform(X_test_imp)
pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index).head()
```

 
:::{.notes} 
Let's bring in our imputed data that we processed in the last slide deck from the California housing dataset. 

At this point, we‚Äôve already imputed the missing data with median values using `SimpleImputer()` and we have not yet transformed it with scaling which we will do now. 
 
We've seen the `StandardScaler()` function earlier but let's see what the transformed data looks like.

Now we can compare our training score with scaled and unscaled data and see a noticeable difference between the two!

Any negative values represent values that are lower than the calculated feature mean and anything positive and greater than 0 are values greater than the original column mean. 
:::

---
 
**Unscaled data**
```{python}
# | output: false
from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor()
knn.fit(X_train_imp, y_train);
print(round(knn.score(X_train_imp, y_train), 3))
print(round(knn.score(X_test_imp, y_test), 3))
```
```out
0.561
0.32
```

<br>

 **Scaled data**
```{python}
# | output: false
knn = KNeighborsRegressor()
knn.fit(X_train_scaled, y_train);
print(round(knn.score(X_train_scaled, y_train), 3))
print(round(knn.score(X_test_scaled, y_test), 3))
```
 ```out
0.798
0.712
 ```
 
:::{.notes}
What about our training scores? 

Just using scaled column values increases the training score by around 30% and the test score increases by around 40%.
:::

---
 
```{python}
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_imp)
X_test_scaled = scaler.transform(X_test_imp)
pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index).head()
```

:::{.notes}
Let's now explore `MinMaxScaler`. 

Note that this time we are using `fit_transform()` syntax. This means we are fitting and immediately transforming the data. We can use this for `StandardScaler()` as well, it just `sklearn` compressing 2 lines of code into a single one. 

In both transformation methods fit is recording certain calculations in order to convert the data. 

For instance, in standardization, during the fit stage, the transforming is learning the mean and standard deviation whereas in normalization, during `fit`, the transforming is learning the minimum and maximum values. 
 
Looking at the data after transforming it with `MinMaxScaler()` we see this time there are no negative values and they all are between 0 and 1.
:::

---
 

**Unscaled data**
```{python}
# | output: false
knn = KNeighborsRegressor()
knn.fit(X_train_imp, y_train);
print(round(knn.score(X_train_imp, y_train), 3))
print(round(knn.score(X_test_imp, y_test), 3))
```

```out
0.561
0.32
```

<br>

**Scaled data**
```{python}
# | output: false
knn = KNeighborsRegressor()
knn.fit(X_train_scaled, y_train);
print(round(knn.score(X_train_scaled, y_train), 3))
print(round(knn.score(X_test_scaled, y_test), 3))
```

```out
0.801
0.723
```

:::{.notes}
Again, similar to `StandardScaler` there is a big difference in the KNN training performance after scaling the data.
 
But we saw in module 3 that the training score doesn't tell us much.
 
We should look at the cross-validation score.
 
Let's take a look at that in the next section.
:::


# Let‚Äôs apply what we learned!
