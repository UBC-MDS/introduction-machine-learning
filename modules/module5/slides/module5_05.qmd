---
format: revealjs
title: Preprocessing with Imputation
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```

## Case study: California housing prices

```{python}
housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)

train_df.head()
```

We are using the data that can be <a href="https://www.kaggle.com/harrywang/housing" target="_blank">downloaded here</a>. 
 
This dataset is a modified version of the California Housing dataset available from: <a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html" target="_blank">Luís Torgo's University of Porto website</a>.

 
 
:::{.notes} 
For the next few slide decks, we are going to be using a dataset exploring the prices of homes in California to demonstrate feature transformation techniques. 

The task is to predict median house values in California districts, given several features from these districts. 

Before we do anything, we load in the data and split it into our train and test splits.

We can see in our training data that we have various districts and information such as where it is, `median_house_age`, `total_bedrooms` etc. Our target columns in the column labeled `median_house_value`. 

Something we need to be aware of is that some column values are mean/median while others are totals or not completely clear. 
:::

---

```{python}
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 

train_df.head()
```

:::{.notes} 
Before we use this data we need to do some **feature engineering**. 

That means we are going to transform our data into features that may be more meaningful for our prediction.

Since we have inconsistent columns, we are going to engineer the new features `rooms_per_household`, `bedrooms_per_household`, and `population_per_household` and remove the columns `total_rooms`, `total_bedrooms`, and `population`.
:::

---

## Exploratory Data Analysis (EDA)

```{python}
train_df.info()
```

:::{.notes} 
After using `.info()` we can we all the different column dtypes and also all the number of null values.

We see that we have all columns with dtype `float64` except for `ocean_proximity` which appears categorical. 
:::

---

```{python}
train_df.describe()
```

<br>

```{python}
train_df["bedrooms_per_household"].isnull().sum()
```

:::{.notes} 
It looks like the training data is missing 185 values for `bedrooms_per_household`.
:::

---

## What happens?

```{python}
X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
```

<br>

```python
knn = KNeighborsRegressor()
knn.fit(X_train, y_train)
```

```out
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
```

:::{.notes} 
First, we are going to drop the categorical variable `ocean_proximity`.  

Right now, we only know how to build models with numerical data. We will come back to the categorical variables in module 6. 

We create our `X` and `y` objects and attempt to run a model. 

Does it work? 

- No.

We can see that the classifier is not able to deal with missing values (NaNs).

What are the possible ways to deal with the problem? 
:::

---

## Dropping

```{python}
train_df["bedrooms_per_household"].isnull().sum()
```

<br>

```{python}
X_train.shape
```

<br>

```{python}
X_train_no_nan = X_train.dropna()
y_train_no_nan = y_train.dropna()

X_train_no_nan.shape
```

 
 
:::{.notes} 
What can we do? 

We could drop the rows but we'd need to do the same in our test set.

That also doesn't help us if we get a missing value in deployment. What do we do then? 

Furthermore, what if the missing values don't occur at random and we're systematically dropping certain data?
Perhaps a certain type of house contributes to more missing values. 

This is not a great solution, especially if there's a lot of missing values.
:::

---

## Dropping a column

```{python}
X_train.shape
```

<br>

```{python}
X_train_no_col = X_train.dropna(axis=1)

X_train_no_col.shape
```

:::{.notes}
One can also drop all columns with missing values.

This generally throws away a lot of information, because we lose a whole column just for 185 missing values out of a total of 18567. 

That means we are throwing away 99% of the column’s data because we are missing 1%.

But dropping a column if it's 99.9% missing values, for example, makes more sense.
:::

---

## Imputation

**Imputation**: Imputation means inventing values for the missing data.

```{python}
from sklearn.impute import SimpleImputer
```

We can impute missing values in:

- **Categorical columns**: with the most frequent value.
- **Numeric columns**: with the mean or median of the column or a constant of our choosing.

:::{.notes} 
`SimpleImputer()` is a **transformer** in `sklearn` which can deal with this problem. 

We are going to concentrate on numeric columns in this section and address categorical preprocessing in Module 6. 
:::

---

```{python}
X_train.sort_values('bedrooms_per_household').tail(10)
```

:::{.notes} 
First, let’s sort the values by `bedrooms_per_household` and we’ll see that the `NaN` values will fall to the end. 

Here we see that the index `7763` has a `NaN` value for `bedrooms_per_household`.
:::

---

```{python}
imputer = SimpleImputer(strategy="median")
imputer.fit(X_train);
X_train_imp = imputer.transform(X_train)
X_test_imp = imputer.transform(X_test)

X_train_imp
```

 
 
:::{.notes} 
Simple import will work by replacing all the `NaN` values in some way, in this case, the column median. 

Let's input our data and instead of dropping the examples, let’s use the `fit` and `transform` steps that we saw earlier.

We fit on the training data and transform it on the train and test splits. 

We do not need to fit on our target column. 

Note that `imputer.transform()` returns a NumPy array and not a dataframe.
:::

---

```{python}
X_train_imp_df = pd.DataFrame(X_train_imp, columns = X_train.columns, index = X_train.index)
X_train_imp_df.loc[[7763]]
```

<br>

```{python}
X_train.loc[[7763]]
```

:::{.notes} 
We are going to convert the output from the transformer into a dataframe so it’s easier to look at. 

Let's check whether the `NaN` values have been replaced or not.

Now we can see our example 7763 no longer has any `NaN` values for the `bedrooms_per_household` now. 
:::

---

```{python}
from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor()
knn.fit(X_train_imp, y_train);
knn.score(X_train_imp, y_train)
```

:::{.notes} 
Can we train on the data with the new data `X_train_imp`?  

Yes! 
:::

# Let’s apply what we learned!
