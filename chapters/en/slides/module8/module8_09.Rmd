---
params:
  dynamictitle: "module8_09"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
library(rsvg)
library(vegawidget) 
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module8/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
import scipy
from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction.text import CountVectorizer


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module8/"
```

type: slides

# Logistic regression

Notes: <br>

---

```{python}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)
X_train, y_train = train_df.drop(columns=["country"], axis=1), train_df["country"]
X_test, y_test = test_df.drop(columns=["country"], axis=1), test_df["country"]

train_df.head()
```


Notes: 

Next, we are going to introduce to you a new model called **logistic regression**. 

It's very similar to `Ridge` we saw earlier but this one has some key differences. 

For one, we can use it with classification instead of regression problems. 

For that reason, we are going to bring back our cities dataset we saw at the beginning of this course. 



---

## Setting the stage

```{python}
from sklearn.dummy import DummyClassifier
dc = DummyClassifier(strategy="prior")

scores= pd.DataFrame(cross_validate(dc, X_train, y_train, return_train_score=True))
scores
```



Notes: 

Although we don't always do this in the slides, we should always be building a baseline model before we do any type of meaningful modeling. 

Let's do that before we get straight into it.

Now we can have a better idea of how well our model performs. 

---

```{python}
from sklearn.linear_model import LogisticRegression
```


```{python}
lr = LogisticRegression()
scores = pd.DataFrame(cross_validate(lr, X_train, y_train, return_train_score=True))
scores
```


Notes: 

We import `LogisticRegression` from the `sklearn.linear_model` library as we did with `Ridge`. 

This time we can see that our training and cross-validation scores have increased from those of our `DummyClassifier`. 



---

## Visualizing our model

```{python echo=FALSE,  fig.width = 13, fig.height = 9,  out.width = '70%', fig.align='center'}
lr.fit(X_train, y_train);
plot_classifier(X_train, y_train, lr, ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 16);
plt.yticks(fontsize= 16);
plt.title("Logistic Regression", fontsize=22)
plt.xlabel("longitude", fontsize=18)
plt.ylabel("latitude", fontsize=18)
```



Notes: 

We saw that with SVMs and decision trees that  we could visualize our model with decision boundaries and we can do the same thing with logistic regression.

Here, we can see we get a line that separates our two target classes. 

---

<br>
<br>
<br>
<br>

```{python include=FALSE,  fig.width = 12, fig.height = 10,  out.width = '100%', fig.align='center'}
models = {
    "KNN": KNeighborsClassifier(),    
    "RBF SVM": SVC(gamma = 0.01),    
    "Logistic Regression": LogisticRegression()
}

plt.figure(figsize=(20, 4))
i = 0
for name, model in models.items():    
    plt.subplot(1, 4, i + 1)
    model.fit(X_train, y_train)
    plot_classifier(X_train, y_train, model, ax=plt.gca())
    plt.xticks();
    plt.yticks();
    plt.title(name, fontsize=14)
    plt.xlabel("longitude")
    plt.ylabel("latitude")
    i += 1
```

<center><img src="/module8/triple_graph.png"  width = "100%" alt="404 image" /></center>

Notes:

If we look at some other models that we did this in comparison for you can understand a bit more on why we call Logistic Regression a "linear Classifiers". 

Notice a linear decision boundary (a line in our case).


---

# Coefficients

```{python}
lr = LogisticRegression()
lr.fit(X_train, y_train); 
```

```python
print("Model coefficients:", lr.coef_)
print("Model intercept:", lr.intercept_)
```

```out
Model coefficients: [[-0.04108149 -0.33683126]]
Model intercept: [10.8869838]
```

```{python}
data = {'features': X_train.columns, 'coefficients':lr.coef_[0]}
pd.DataFrame(data)
```


Notes: 

Just like we saw for `Ridge`. we can get the equation of that line and the coefficients of our `latitude` and `longitude` features using `.coef_`.

In this case, we see that both are negative coefficients. 

We also can see that the coefficient of latitude is larger in magnitude than that of longitude. 

This makes a lot of sense because Canada as a country lies above the USA and so we expect `latitude` values to contribute more to a prediction than `longitude` which Canada and the `USA` have quite similar values. 


---

## Predictions

```{python}
lr.classes_
```


```{python}
example = X_test.iloc[0,:]
example.tolist()
```


```{python}
(example.tolist() * lr.coef_).sum(axis=1) + lr.intercept_ 
```

```{python}
lr.predict([example])
```


Notes: 

Again, let's take an example from our test set and calculate the outcome using our coefficients and intercept. 

We get a value of -1.978. 

In `Ridge` our prediction would be the calculated result so -1.97, but for logistic regression, we check the **sign** of the calculation only.

Our threshold is 0.

- If the result was positive, it predicts one class; if negative, it predicts the other.

That means everything negative corresponds to "Canada" and everything positive predicts a class of "USA". 

If we use `predict`, it gives us the same result as well! 

These are "hard predictions" but we can also use this for something called "soft predictions" as well.

(That's in the next slide deck!)

---

## Hyperparameter: C (A new one)


```{python }
scores_dict ={
"C" :10.0**np.arange(-6,2,1),
"train_score" : list(),
"cv_score" : list(),
}
for C in scores_dict['C']:
    lr_model = LogisticRegression(C=C)
    results = cross_validate(lr_model, X_train, y_train, return_train_score=True)
    scores_dict['train_score'].append(results["train_score"].mean())
    scores_dict['cv_score'].append(results["test_score"].mean())
```

```{python}
pd.DataFrame(scores_dict)
```


Notes: 

At this point, you should be feeling pretty comfortable with hyperparameters. 

We saw that `Ridge` has the hyperparameter `alpha`, well `C` (annoyingly) has the opposite effect on the fundamental trade-off. 

In general, we say smaller `C` leads to a less complex model (whereas with `Ridge`, lower `alpha` means higher complexity). 

Higher values of `C` leads to more overfitting and lower values to less overfitting. 



---

```{python include =FALSE}
plotting_source = pd.DataFrame(scores_dict).melt(id_vars='C', 
                                  value_vars=['train_score', 'cv_score'], 
                                  var_name='score_type' ,
                                  value_name= 'accuracy' )
                                  
                                  
mss_acc_plot = alt.Chart(plotting_source, width=500, height=300).mark_line().encode(
    alt.X('C:Q', scale=alt.Scale(type='log')),
    alt.Y('accuracy:Q', scale=alt.Scale(domain=[0.55, 0.9])),
    alt.Color('score_type:N')
).properties(title="Accuracies of C split for logistic regression").to_json()

```

```{r include=FALSE, fig.width = 13, fig.height = 8,  out.width = '80%', fig.align='center'}
as_vegaspec(py$mss_acc_plot)
``` 

```{python}
param_grid = {
    "C": scipy.stats.uniform(0, 100)}

lr = LogisticRegression()
grid_search = RandomizedSearchCV(lr, param_grid, cv=5, return_train_score=True, verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train);
``` 

```{python}
grid_search.best_params_
grid_search.best_score_
```


Notes: 

`LogisticRegression`'s default `C` hyperparameter is 1. 

Let's see what kind of value we get if we do `RandomizedGrid`. 

---

## Logistic regression with text data


```{python}
X = [
    "URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!",
    "Lol you are always so convincing.",
    "Nah I don't think he goes to usf, he lives around here though",
    "URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!",
    "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030",
    "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"]

y = ["spam", "non spam", "non spam", "spam", "spam", "non spam"]
```




Notes: 

In one of the practice problems and in the assigment, we apply logistic regression with text data.

We want to give you a bit of background for this.

Let's bring back our spam dummy data that we looked at in Modulee 6. 



---


```{python}
vec = CountVectorizer()
X_transformed = vec.fit_transform(X);
bow_df = pd.DataFrame(X_transformed.toarray(), columns=sorted(vec.vocabulary_), index=X)
bow_df
```


Notes: 

`CountVectorizer` transforms our `review` column into multiple columns each being a word from the `X` vocabulary. 



---


```{python}
lr_text_model = LogisticRegression()
lr_text_model.fit(X_transformed, y);
```

```{python}
pd.DataFrame({'feature': vec.get_feature_names(),
              'coefficient': lr_text_model.coef_[0]})
```


Notes: 

That means that each  word is a feature in our model and therefore when we apply logistic regression to our feature table, we get a coefficient for each word! 

This should help you in understanding how the coefficients contribute to the predictions of each example for both the practice problems and the assignment. 


---

# Let’s apply what we learned!

Notes: <br>
