---
params:
  dynamictitle: "module8_05"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module8/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import make_scorer
from sklearn.linear_model import Ridge

pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module8/"
```


type: slides

# Coefficients and coef_

Notes: <br>

---

## Intuition behind linear regression


| listing number | Number of Bedrooms | Number of Bathrooms | Square Footage | Age | Price |
|----------------|--------------------|---------------------|----------------|-----|-------| 
| 1              | 5                  | 6                   | 3000           | 2   | $6.39 million|
| 2              | 1                  | 1                   | 800            | 90  | $1.67 million|
| 3              | 3                  | 2                   | 1875           | 66  | $3.92 million|

Notes:

Unlike with decision trees where we make predictions with rules and analogy-based models where we predict a certain class using distance to other examples, linear classifiers use **coefficients** (or sometimes known as "weights") associated with features.




---

<center><img src="/module8/house_table.png"  width = "50%" alt="404 image" /></center>

Notes: 


We then use these learned coefficients to make predictions.

For example, suppose we are predicting the price of a house and we have 4 features;  number of bedrooms, number of bathrooms, square footage, and age. 

---

<center><img src="/module8/house_table.png"  width = "35%" alt="404 image" /></center>


Consider the following listing (example):     

| listing number | Number of Bedrooms | Number of Bathrooms | Square Footage | Age |
|----------------|--------------------|---------------------|----------------|-----| 
| 3              | 3                  | 2                   | 1875           | 66  |

<br>

<font size="4"><em> predicted(price) = coefficient<sub>bedrooms</sub> x #bedrooms + coefficient<sub>bathrooms</sub> x #bathrooms + coefficient<sub>sqfeet</sub> x #sqfeet + coefficient<sub>age</sub> x #age + intercept </em></font>


<font size="4"><em> predicted(price) = 0.03 x #bedrooms + 0.04 x #bathrooms + 0.003 x #sqfeet + -0.01 x #age + intercept </em></font>


<font size="4"><em> predicted(price) = (0.03 x 3) + (0.04 x 2) + (0.003 x 1875) + (-0.01 x 66) + 0 </em></font>

<font size="4"><em> predicted(price) = 3.26 </em></font>


Notes: 



---

## Components of a linear model

<br>
<br>
<br>

<font size="5"><em> predicted(price) = (<font  color="#b1d78c">coefficient<sub>bedrooms</sub></font>   x  <font  color="7bd1ec">#bedrooms</font>)   +  (<font  color="#b1d78c">coefficient<sub>bathrooms</sub></font>  x  <font  color="7bd1ec">#bathrooms</font>)  +  (<font  color="#b1d78c">coefficient<sub>sqfeet</sub></font>   x   <font  color="7bd1ec">#sqfeet</font>)  +  (<font  color="#b1d78c">coefficient<sub>age</sub></font>  x <font  color="7bd1ec">#age</font>)  +  <font  color="e8b0d0">intercept</font> </em> </font> 

- <font  color="7bd1ec"> Input features</font>     
- <font  color="#b1d78c"> Coefficients, one per feature</font>      
- <font  color="e8b0d0"> Bias or intercept</font> 



Notes: 


---


```{python}
housing_df = pd.read_csv("data/real_estate.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=1)
train_df.head()
```

```{python}
X_train, y_train = train_df.drop(columns =['price']), train_df['price']
X_test, y_test = test_df.drop(columns =['price']), test_df['price']
```



Notes: 

Let's now use `Ridge` with our Taiwan housing dataset again from the last slide deck. Here, we want to predict the house price. 




---

```{python}
lm = Ridge()
lm.fit(X_train, y_train);
training_score = lm.score(X_train, y_train)
training_score
```

```{python}
lm.coef_
```

Notes: 

We can make our model as usual and train it, and assess our training score.

We saw that with linear classifiers we have coefficients associated with each feature of our model. 

How do we get that? We can use `.coef_` to obtain them from our trained model. 

But how are these useful? 


---

```{python}
ridge_coeffs = lm.coef_
ridge_coeffs
```

```{python}
words_coeffs_df = pd.DataFrame(data=ridge_coeffs, index=X_train.columns, columns=['Coefficients'])
words_coeffs_df
``` 





Notes: 

One of the primary advantages of linear classifiers is their ability to interpret models using these coefficients. 

What do these mean? Let's try to make some sense of it here. 

We have our coefficients but we should see which feature corresponds to which coefficient. 

We can do that by making a dataframe with both values. 

We can use these coefficients to interpret our model. They show us how much each of these features affects our model's prediction.

For example, if we had a house with 2 stores nearby, our `num_stores` value is 2. That means that 2 * 1.26 = 2.52 will contribute to our predicted price!

The negative coefficients work in the opposite way, for example, every unit increase in age of a house will, subtracts 0.244 from the house's predicted value.


---

```{python}
words_coeffs_df.abs().sort_values(by='Coefficients')
```

Notes:

In linear models, the coefficients tell us how each feature affects the prediction. 

So, looking at the features which have coefficient with bigger magnitudes might be useful and contribute more to the prediction. 

It's important to be careful here though because this depends on the scaling of the features. Larger features will have smaller coefficients, but if we scale our features before we build our model then they are on a somewhat level playing field! (Another reason we should be scaling our features!)

---

## Interpreting learned coefficients

<br>
<br>

In linear models: 

- if the coefficient is +, then ↑ the feature values ↑ the prediction value.  
- if the coefficient is -, then ↑ the feature values ↓ the prediction value.  
- if the coefficient is 0,  the feature is not used in making a prediction.



Notes:

In linear models: 

- if the coefficient is positive, then increasing the feature values increases the prediction value.  
- if the coefficient is negative, then increasing the feature values decreases the prediction value.  
- if the coefficient is zero, the feature is not used in making a prediction

---

## Predicting


```{python}
X_train.iloc[0:1]
```

```{python}
lm.predict(X_train.iloc[0:1])
```



Notes: 

Let's take a look at a single example here. 

The values in this are the input features. 

We can use `predict()` on our features to get a prediction of 52.36.



---



```{python}
words_coeffs_df.T
```

```{python}
X_train.iloc[0:1]
```

```{python}
intercept = lm.intercept_
intercept
```



Notes: 

Using our coefficients, and the model's intercept we can calculate the model's predictions ourselves as well. 




---

<center><font size="4"><em> predicted(price) = coefficient<sub>house_age</sub> x house_age + coefficient<sub>distance_station</sub> x distance_station + coefficient<sub>num_stores</sub> x num_stores + coefficient<sub>latitude</sub> x latitude +  coefficient<sub>longitude</sub> x longitude + intercept </em></font></center>


```{python}
(ridge_coeffs * X_train.iloc[0:1]).sum(axis=1) + intercept 
```


```{python}
lm.predict(X_train.iloc[0:1])
```
Notes:

All of these feature values multiplied by the coefficients then adding the intercept, contribute to our prediction. 

When we do this by hand using the model's coefficients and intercept, we get the same as if we used `predict`.

---

# Let’s apply what we learned!

Notes: <br>
