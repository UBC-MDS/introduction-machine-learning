---
params:
  dynamictitle: "module8_17"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module8/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets
from sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression

pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module8/"
```


type: slides

# Multi-class classification

Notes: <br>

---

```{python}
data = datasets.load_wine()
X = pd.DataFrame(data['data'], columns=data["feature_names"])
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)
```

```{python}
X_train
```

```{python}
y_train
```


Notes: 

The classification problems we have looked at so far in this module have had binary labels (2 possible labels).

But we've seen that target label's are not restricted to this. Often we will have classification problems where we have multiple labels such as this wine dataset we are going to use.

Here we have 3 classes for our target: 0, 1, 2. (maybe red, white and rose?)

---

```{python}
lr = LogisticRegression(max_iter=10000)
lr.fit(X_train, y_train);
```

```{python}
lr.predict(X_test[:5])
```


```{python}
lr.coef_
```



Notes: 

For some models, like a decision trees, we don't have to think about anything differently at all, but for our linear classifier, on the other hand, things are a bit different. 

What is going on here?

---

```{python}
lr.coef_.shape
```
```{python}
lr_coefs = pd.DataFrame(data=lr.coef_.T, index=X_train.columns, columns=lr.classes_)
lr_coefs
```

```{python}
lr_coefs.loc["alcohol", 1]
```


Notes: 

What's happening here is that we have one coefficient per feature *per class*.


The interpretation is a feature importance for predicting a certain class. 

This means that if alcohol is larger , it's more likely to predict class 1. 

The specific interpretation depends on the way the logistic regression is implementing multi-class.

---



```{python}
lr.predict_proba(X_test)[:5]
```

```{python}
lr.predict_proba(X_test[:5]).sum(axis=1)
```


Notes: 

If we look at the output of `predict_proba` you'll also see that there is a probability for each class and each row adds up to 1 as we would expect (total probability = 1).

We can also sum them up per row and we can see they add up to 1 as well. 


---

```{python}
confusion_matrix(y_test, lr.predict(X_test))
```



```{python  fig.width = 10, fig.height = 8,  out.width = '60%', fig.align='center'}
plot_confusion_matrix(lr, X_test, y_test, display_labels=lr.classes_, cmap='Blues', values_format='d');
plt.show()
```


Note: Like we saw in Module 7, we can still create confusion matrices but now they are greater than a 2 X 2 grid. 

---


```{python}
print(classification_report(y_test, lr.predict(X_test)))
```

Notes: 

Precision, recall, etc. don't apply directly.
But like we said before, if we pick one of the classes as positive, and consider the rest to be negative, then we can.


---

```{python}
x_train_2d = X_train[['alcohol','malic_acid']]
x_train_2d.head()
```



```{python  echo=FALSE, fig.width = 13, fig.height = 8,  out.width = '90%', fig.align='center'}
lr_2d = LogisticRegression()
lr_2d.fit(x_train_2d, y_train);
plot_classifier(x_train_2d, y_train, lr_2d,  ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 14);
plt.yticks(fontsize= 14);
plt.xlabel("alcohol", fontsize=14); plt.ylabel("malic_acid", fontsize=14)
plt.title("Logistic Regression - Multi Class", fontsize=16)
```



Notes: 

We can also make the plots we made earlier in the course, but this time with more classes.

In order for us to be able to plot this we need to select 2 features so we are picking `alcohol` and `malic_acid`. 

In this plot the colours are inconsisten wit the shapes. 
- The red triangles correspond to the light blue predictions.
- The black X's correspond to the red predictions/
- The blue circles (correctly) correspond to the blue circles.


---



```{python  echo=FALSE, fig.width = 12, fig.height = 8,  out.width = '90%', fig.align='center'}
plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1);
rf_2f = DecisionTreeClassifier();
rf_2f.fit(X_train.iloc[:,:2], y_train);
plot_classifier(x_train_2d, y_train, rf_2f, ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 14);
plt.yticks(fontsize= 14);
plt.xlabel("alcohol", fontsize=14); plt.ylabel("malic_acid", fontsize=14)
plt.title("Decision Tree - Multi Class", fontsize=16)
plt.subplot(1, 2, 2);
svm_2f = SVC(gamma=2, C=100)
svm_2f.fit(X_train.iloc[:,:2], y_train);

plot_classifier(x_train_2d, y_train, svm_2f, ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 14);
plt.yticks(fontsize= 14);
plt.xlabel("alcohol", fontsize=14); plt.ylabel("malic_acid", fontsize=14)
plt.title("SVM - Multi Class", fontsize=16)

```

```{python}
plot_classifier(x_train_2d, y_train, lr_2d,  ax=plt.gca(), ticks=True)
plt.xticks(fontsize= 14);
plt.yticks(fontsize= 14);
plt.xlabel("latitude", fontsize=14); plt.ylabel("logitude", fontsize=14)
plt.title("Logistic regression -multi classifications", fontsize=16)
```


Notes: 

We can plot multi-class problems with other classifiers too. Here we can see the boundaries of the decision tree classifier as well as SVM with an RBF kernel. 
---

# Letâ€™s apply what we learned!

Notes: <br>