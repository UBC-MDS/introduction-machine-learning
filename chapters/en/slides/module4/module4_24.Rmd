---
params:
  dynamictitle: "module4_24"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module4/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import train_test_split, cross_validate
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module4/"
```


type: slides

# Support Vector Machines (SVMs) with RBF kernel

Notes: <br>

---

```{python}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)
train_df.head()
X_train, y_train = train_df.drop(columns=['country']), train_df['country']
X_test, y_test = test_df.drop(columns=['country']), test_df['country']
```

```{python}
train_df.head()
```

Notes: 

Another popular similarity-based algorithm is Support Vector Machines (SVM).

SVMs use a different similarity metric which is called a "kernel" in SVM land. 

We are going to concentrate on the specific kernel called Radial Basis Functions (RBFs).

Let's bring back our trusty cities dataset again. 




---


```{python}
cities_plot = alt.Chart(train_df).mark_circle(size=20, opacity=0.6).encode(
    alt.X('longitude:Q', scale=alt.Scale(domain=[-140, -40])),
    alt.Y('latitude:Q', scale=alt.Scale(domain=[20, 60])),
    alt.Color('country:N', scale=alt.Scale(domain=['Canada', 'USA'],
                                           range=['red', 'blue'])))
cities_plot
```

```{python include =FALSE}
cities_plot.save(path + 'cities_plot.png')
```

<img src="/module4/cities_plot.png" alt="A caption" width="60%" />



Notes: 

Here is our data plotted once again with the red dots representing Canadian cities and the blue ones represent American cities. 

---

```{python}
from sklearn.svm import SVC
```


```{python}
svm = SVC(gamma=0.01)
scores = cross_validate(svm, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```

```{python}
svm_cv_score = scores['test_score'].mean()
svm_cv_score
```


Notes: 

In this course, we are not going into detail about how support vector machine classifiers or regressor works but more so on how to use it with Scikit-learn. 

We must first import the necessary library and then we can get to model building in the same fashion as we did before.  

Here we are importing the `SVC` tool from the `sklearn.svm` library. 

For now, just ignore the gamma input argument in `SVC()` we will get to that soon. 

After building our model and performing cross-validation, we can see that the mean accuracy is 0.820. 


---

```{python}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)
X_train, y_train = train_df.drop(columns=['country']), train_df['country']
X_test, y_test = test_df.drop(columns=['country']), test_df['country']
```


```{python include=FALSE}
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train);
scores = cross_validate(knn, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```


```{python echo=FALSE, fig.width = 10, fig.height = 8,  out.width = '100%'}
svm.fit(X_train, y_train);
plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plt.title("SVC")
plot_classifier(X_train, y_train, svm, ax=plt.gca())
plt.subplot(1, 2, 2)
plt.title("KNN with k = 5")
plot_classifier(X_train, y_train, knn, ax=plt.gca())
```


Notes: 

If we plot over the support vector machine classifier along with the ùëò-Nearest Neighbours classifier, we can see that the support vector machine classifier is a smoothed version of the ùëò-Nearest Neighbours classifier. 

---

### SVMs

```{python}
knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_validate(knn, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```

```{python}
knn_cv_score = scores['test_score'].mean().round(3)
knn_cv_score
```

```{python}
svm_cv_score.round(3)
```

Notes: 

Superficially, support vector machines are very similar to ùëò-Nearest Neighbours.

A test example is positive if on average it looks more like positive examples it is negative if on average it looks more like negative examples.

The primary difference between ùëò-NNs and SVMs is that:

- Unlike ùëò-NNs, SVMs only remember the key examples (support vectors). 
- When it comes to predicting a query point, we only consider the key examples from the data, and only calculate the distance to these key examples. This makes it more efficient than ùëò-NN. 

If we compare the scores from the ùëò-NN model using `n_neighbors=5` and the scores from the SVM model we get similar results, however, the SVM model seems to do slightly better than the ùëò-NN model. 

---

## SVM Regressor 


```{python}
from sklearn.svm import SVR
```



Notes:

It should come as no surprise that we can use SVM models for regression problems as well.

We need to make sure to import SVR from the SVM sklearn library. 



---

<br>
<br>

### Hyperparameters of SVM are:

- `gamma`
- `C`
    


Notes: 

There are 2 main hyperparameters for support vector machines with an RBF kernel; `gamma` and `C`.

We are not going into detail about the interpretation of these hyperparameters but we will observe how they are related to the fundamental trade-off. 

If you wish to learn more on these you can reference <a href="https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html" target="_blank">Scikit-learn`'s explanation of RBF SVM parameters</a>.



---

### Relation of gamma and the fundamental trade-off

`gamma` controls the complexity of a model, just like other hyperparameters we've seen.

- As  gamma  ‚Üë, complexity  ‚Üë
- As  gamma  ‚Üì, complexity ‚Üì

<br>
<br>
<br>

```{python  echo=FALSE, fig.width = 20, fig.height = 12,  out.width = '100%'}
plt.figure(figsize=(16, 4))
for i in range(4):
    plt.subplot(1, 4, i + 1)
    gamma = 10.0 ** (i - 3)
    rbf_svm = SVC(gamma=gamma)
    rbf_svm.fit(X_train, y_train)
    plt.title("gamma = %s" % gamma);
    plot_classifier(X_train, y_train, rbf_svm, ax=plt.gca(), show_data=False)
```


Notes: 


The first type of hyperparameter is `gamma`. `gamma` controls the complexity of the model.

Using higher values for `gamma` means a more complex model is produced whereas lower values result in a less complex model. 

If we look at the plots, it appears that with lower values of gamma, the model is likely underfitting, and as gamma increases, the potential of overfitting is also increasing.


---

### Relation of C and the fundamental trade-off
\
C also affects the fundamental tradeoff.

- As  C  ‚Üë, complexity  ‚Üë
- As  C  ‚Üì, complexity ‚Üì

<br>
<br>
<br>

```{python  echo=FALSE, fig.width = 20, fig.height = 16,  out.width = '100%'}
plt.figure(figsize=(16, 4))
for i in range(4):
    plt.subplot(1, 4, i + 1)
    C = 10.0 ** (i - 1)
    rbf_svm = SVC(C=C, gamma=0.01)
    rbf_svm.fit(X_train, y_train)
    plt.title("C = %s" % C);
    plot_classifier(X_train, y_train, rbf_svm, ax=plt.gca(), show_data=False)
    
```


Notes: 

The other hyperparameter we will look at is `C`. `C` also controls the fundamental trade-off. Just like with gamma, higher values increase the model complexity whereas lower values decrease the complexity. 

Obtaining optimal validation scores requires a hyperparameter search between both `gamma` and `C` to balance the fundamental trade-off.

We will learn how to search over multiple hyperparameters at a time in the next module. 


---

# Let‚Äôs apply what we learned!

Notes: <br>