---
params:
  dynamictitle: "module4_17"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module4/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_validate

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module4/"
```


type: slides

# Choosing K (n_neighbors)

Notes: <br>

We saw that the prediction of over query pointing changes with different values for the `n_neighbors` argument.

So, a natural question is _how do we pick `n_neighbors`?_. 

What happens when we change this hyperparameter? 

Are we likely to be overfitting or underfitting with higher or lower values of `n_neighbors`?



---

```{python}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
X = cities_df.drop(columns=["country"])
y = cities_df["country"]
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=123)
```

```{python}
model = KNeighborsClassifier(n_neighbors=1)
model.fit(X_train, y_train.to_numpy());
```


```{python}
model.score(X_train,y_train)
```

Notes: 

Let's examine this using our cities data. 

As usual, we create our `X` and `y` objects as well as our training and test splits.

We create our `KNeighborsClassifier` object with `n_neighbors=1` and training the model. 

When we score it, we get an accuracy of 1 on the training data. 




---

```{python}
k = 1
knn1 = KNeighborsClassifier(n_neighbors=k)
scores = cross_validate(knn1, X_train, y_train, return_train_score = True)
pd.DataFrame(scores)
```

```{python}
k = 100
knn100 = KNeighborsClassifier(n_neighbors=k)
scores = cross_validate(knn100, X_train, y_train, return_train_score = True)
pd.DataFrame(scores)
```

Notes: 

Let's carry out cross-validation with ùëò=1. 

These are our cross-validation results. 

What we see here is in each fold our training score always produces a perfect accuracy of 1.0.

Our validation score for each fold is much lower than the training score. 

The gap between the training and validation sets seems to be high and so it is likely that our model is overfitting. 

Let‚Äôs seen now what happens when ùëò=100. 

Now, we see that our training scores are much lower and our validation scores are also lower. 

The gap between the training and validation sets seem to be lower. This looks like our model is underfitting now. 




---

```{python echo=FALSE}
plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
knn1.fit(X_train, y_train);
plt.title("n_neighbors = 1")
plt.ylabel("latitude")
plt.xlabel("longitude")
plot_classifier(X_train, y_train, knn1, ax=plt.gca(), ticks=True)


plt.subplot(1, 2, 2)
plt.title("n_neighbors = 100")
knn100.fit(X_train, y_train);
plt.ylabel("latitude")
plt.xlabel("longitude")
plot_classifier(X_train, y_train, knn100, ax=plt.gca(), ticks=True)
```

Notes: 

If we plot these two models with ùëò=1 on the left and ùëò=100 on the right. 

The left plot shows a much more complex model where it is much more specific and attempts to get every example correct. 

The plot on right is plotting a simpler model and we can see more training examples are being predicted incorrectly. 

---

### How to choose `n_neighbors`?

```{python}
results_dict = {"n_neighbors": list(), "mean_train_score": list(), "mean_cv_score": list()}

for k in range(1,50,5):
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_validate(knn, X_train, y_train, return_train_score = True)
    results_dict["n_neighbors"].append(k)
    results_dict["mean_cv_score"].append(np.mean(scores["test_score"]))
    results_dict["mean_train_score"].append(np.mean(scores["train_score"]))

results_df = pd.DataFrame(results_dict)
results_df
```



Notes: 

In our toy problem with ùëò=1, we saw the model was overfitting yet when ùëò=100, the model was underfitting. 

So, the question is how do we pick ùëò?

- The answer lies in hyperparameter optimization. 

Here we are looping over different values of ùëò ( `n_neighbors`) and performing cross-validation on each one. 



---

```{python include =FALSE}

plotting_source = results_df.melt(id_vars='n_neighbors', 
                                  value_vars=['mean_train_score', 'mean_cv_score'], 
                                  var_name='score_type' ,
                                  value_name= 'accuracy' )
                                  
                                  
K_plot = alt.Chart(plotting_source, width=500, height=300).mark_line().encode(
    alt.X('n_neighbors:Q'),
    alt.Y('accuracy:Q', scale=alt.Scale(domain=[.67, 1.00])),
    alt.Color('score_type:N')
).properties(title="Accuracies of n_neighbors for KNeighborsClassifier")


```

```{python include =FALSE}
K_plot.save(path + 'K_plot.png')
```

<br>
<center><img src="/module4/K_plot.png" alt="A caption" width="80%" /></center>


Notes: 

In this graph we‚Äôve plotted, `n_neighbors` is on the x-axis and the model accuracy is on the y-axis.

We can see there is a sweet spot where the gap between the validation and training scores is the lowest. Here it‚Äôs when `n_neighbors` is 11.


---


```{python}
sorted_results_df = results_df.sort_values("mean_cv_score", ascending = False)
sorted_results_df
```

```{python}
best_k = sorted_results_df.iloc[0,0]
best_k
```

Notes: 

We can find the most optimal `n_neighbors` value by sorting our results on the mean validation score. 

This shows the best validation score occurs when `n_neighbors=11`.

---

```{python}
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train);
print("Test accuracy:", round(knn.score(X_test, y_test), 3))
```


Notes: 

Now that we know the best scoring hyperparameter value, we are ready to assess our model on the test set. 

We recreate our model with `n_neighbors=11`, fit the model and score it on the test set. 

Our testing accuracy is 0.905 which is higher than the validation mean accuracy we had earlier. 

This is surprising and could be due to having a small dataset. 


---

### Curse of dimensionality


<br> 

- ùëò -NN usually works well when the number of dimensions is small.

<br>
<br>


<center><img src="/module4/skull.png" alt="A caption" width="60%" /></center>



Notes:


In the previous module, we discussed one of the most important problems in machine learning which was overfitting the second most important problem in machine learning is the ***curse of dimensionality***.

This problem affects most models but this problem is especially bad for ùëò-NN. 

ùëò-NN works well then the number of dimensions is small but things fall apart fairly quickly as the number of dimensions goes up. 

If there are many irrelevant features, ùëò-NN is hopelessly confused because all of them contribute to finding similarities between examples. 

With enough irrelevant features, the accidental similarity between features wipe out any meaningful similarity and ùëò-NN becomes is no better than random guessing. 


---

### Other useful arguments of `KNeighborsClassifier`


<center><img src="/module4/knn.png" alt="A caption" width="80%" /></center>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" target="_blank">**Attribution** </a>  





Notes: 

Another useful hyperparameter is `weight`. 

So far, when predicting labels, we have been giving equal weight to all the nearby examples. 

We can change that using this `weight` hyperparameter. 

We can tell it to weigh the examples higher if they are closer to the query point. 


---

# Let‚Äôs apply what we learned!

Notes: <br>