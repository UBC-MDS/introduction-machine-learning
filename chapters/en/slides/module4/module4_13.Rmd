---
params:
  dynamictitle: "module4_14"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module4/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
from sklearn.model_selection import cross_validate

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module4/"
```


type: slides

# ùëò -Nearest Neighbours (ùëò-NNs) Classifier

Notes: <br>

Now that we have learned how to find similar examples, how can we use this idea in a predictive model?

---


<br>
<br>
<center><img src="/module4/scatter.png"  width = "50%" alt="404 image" /></center>


Notes: 

Let's demonstrate this using some toy data for binary classification. 

We have two features in our toy example; feature 1 and feature 2. 

We have two targets; 0 represented with blue points and 1 represented with orange points. 

We want to predict the point in gray. 

Based on what we have been doing so far, we can find the closest example to this gray point and use its class as the class for our grey point. 



---

<br>
<br>
<center><img src="/module4/scatter_k1.png"  width = "50%" alt="404 image" /></center>


Notes: 

In this particular case, we will predict orange as the class for our query point. 

---

<br>
<br>
<center><img src="/module4/scatter_k3.png"  width = "50%" alt="404 image" /></center>

Notes: 

Now you might be thinking that it may not be a great idea to make this decision based only on one nearest example. 

What if we consider more than one nearest example and let them vote on the target of the query example. 

In our toy example, we can consider the three closest points and let them vote. 

Now our prediction changes. 

Before our prediction was orange with only one nearest point and with three nearest points, now our prediction is blue. 


---

```{python include=FALSE}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)
train_df.head()
```

```{python}
small_train_df = cities_df.sample(30, random_state=90)
X_train = small_train_df.drop(columns=["country"])
y_train = small_train_df["country"]
one_city = small_train_df.sample(1, random_state=44)
one_city
```

<center><img src="/module4/point.png"  width = "63%" alt="404 image" /></center>

Notes: 

Let's look at how we can do this using Sklearn. 

Let's go back to our cities data. Here we are only considering thirty examples from our cities data. 

We are sampling 1 point from our data and that is the point represented with the green triangle. 


----

<center><img src="/module4/point.png"  width = "60%" alt="404 image" /></center>



```{python}
from sklearn.neighbors import KNeighborsClassifier

neigh = KNeighborsClassifier(n_neighbors=1)
neigh.fit(X_train, y_train);
neigh.predict(one_city.drop(columns=["country"]))

```


Notes: 

Our goal is to find the class for this green triangle example.  

As usual, we will import the necessary libraries (here we are importing the `KneighborsClassifier` function) then we create our class object. 

We create our class object with only one neighbour. We can do this by passing one as the value for this `n_neighbors` argument. 

We fit the classifier on our training data and we predict the single green triangle city. 

Our prediction here is Canada since the closest point to the green triangle is a city with the class ‚ÄúCanada‚Äù.


---


<center><img src="/module4/point.png"  width = "60%" alt="404 image" /></center>


```{python}
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train, y_train);
neigh.predict(one_city.drop(columns=["country"]))

```


Notes: 

Now, what if we consider the nearest 3 neighbours?

We can do this by passing three to the `n_neighbors` argument. 

We fit on the model. 

Using `predict` on our new model still gives us a classification of ‚ÄúCanada‚Äù. 


---

<center><img src="/module4/point.png"  width = "60%" alt="404 image" /></center>


```{python}
neigh = KNeighborsClassifier(n_neighbors=9)
neigh.fit(X_train, y_train);
neigh.predict(one_city.drop(columns=["country"]))

```


Notes: 

When we change our model to consider the nearest 9 neighbours, our prediction changes!

It now predicts "USA" since the majority of the 9 nearest points are "USA" cities. 

---

```{python include=FALSE}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
X = cities_df.drop(columns=["country"])
y = cities_df["country"]
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123)
```

```{python}
model = KNeighborsClassifier(n_neighbors=1)
model.fit(X_train, y_train);
```


```{python}
model.score(X_train,y_train)
```
```{python}
model.score(X_test,y_test)
```


Note: 

Let's score our model with 1 neighbour. 

We create our model and fit it.

We see that with one neighbour we get the perfect score of 1.0 on the training data. 

But our test score is much lower. When we score our model 0.714 accuracy. 



---

# Let‚Äôs apply what we learned!

Notes: <br>