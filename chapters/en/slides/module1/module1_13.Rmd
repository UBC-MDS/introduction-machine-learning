---
params:
  dynamictitle: "module1_07"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module1/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from image_classifier import classify_image

from IPython.display import HTML, display
from PIL import Image, ImageFile
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

```


type: slides

# Baselines: Training a Model using Scikit-learn

Notes: <br>

---
 
### Supervised Learning (Reminder)

- Tabular data â†’ Machine learning algorithm â†’ ML model â†’ new examples â†’ predictions


<center>
<img src="/module1/sup-learning.png" height="1200" width="1200"> 
</center>

Notes: 

Just to recap what we know, we take tabular data and a machine learning algorithm and produce a machine learning model. 

We can then take new examples and make predictions on them using this model. 

---

### Building a simplest machine learning model using sklearn

<br>
<br>
<br>
Baseline model:
 **most frequent baseline**: always predicts the most frequent label in the training set.



Notes: 

Let's build a ***baseline*** simple machine learning algorithm based on simple rules of thumb. 

We are going to build a  most frequent baseline model which always predicts the most frequent label in the training set. 

Baselines provide a way to sanity check your machine learning model.  


---

## Data 

```{python}
classification_df = pd.read_csv("data/quiz2-grade-toy-classification.csv")
classification_df.head()
```



Notes: 

Let's take our data. 

For this example, we are going to be working with the quiz2 classification data that we have seen previously. 

---

## 1. Create  ğ‘‹  and  ğ‘¦

ğ‘‹  â†’ Feature vectors <br>
ğ‘¦  â†’ Target

```{python}
X = classification_df.drop(["quiz2"], axis=1)
y = classification_df["quiz2"]
```


Notes: 

Our first step in building our model is spliting up our tabular data into the features and the target, also known as ğ‘‹ and ğ‘¦. 

ğ‘‹ is all of our features in our data, which we also call our ***feature table***.
ğ‘¦ is our target, which is what we are predicting.

For this problem, all the columns in our dataframe except  `quiz2`  make up our ğ‘‹ and the `quiz2` column, which is our target make up  our ğ‘¦. 

<br>

---

## 2. Create a classifier object

- `import` the appropriate classifier. 
- Create an object of the classifier. 

```{python}
from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy="most_frequent")
```


Notes: 

In order to make our baseline model, we need to import the necessary library.

We spoke about the Scikit Learn package in the last slide deck. 

Here we are importing `DummyClassifier()` which will be used to create our baseline model. 

We specify in the `strategy` argument `most_frequent` which means our model will always predicts the most frequent label in the training set. 

Here we are naming our model `dummy_clf`.



---

## 3. Fit the classifier

```{python results = 'hide'}
dummy_clf.fit(X, y)
```


Notes: 

Once we have picked and named our model, we give it data to train on. 

The model's "learning" is carried out when we call `fit` on the classifier object. 

In a lot of models, the fitting (also know as the training) stage takes the longest and is where most of the work occurs. This isn't always the case but it is in a lot of them. 


---

## 4. Predict the target of given examples

We can predict the target of examples by calling `predict` on the classifier object. 

Let's see what it predicts for a single observation first: 
```{python}
single_obs = X.loc[[0]]
single_obs
```

```{python}
dummy_clf.predict(single_obs)
```






Notes: 

Now that our model has been trained on existing data, we can predict the targets.

We are going to try to predict on data that the model has already seen. In this case `X`, was used to in the fitting stage. This will change very soon. 

Let's first see what the model predicted for a single observation. 

We can see here, that for observation 0, it's predicting a value of `not A+`. 

This was the most frequent `quiz2` value in the data that we gave it during the `.fit()` stage. 



---

```{python}
X
```



```{python}
dummy_clf.predict(X)
```





Notes:

And if we see the predictions for all the observations in `X`, the model predicts a value of `not A+` for each one.

We will talk more about `.fit()` and `.predict()` in the next module.  

---

## 5. Scoring your model

In the classification setting, `.score()` gives the accuracy of the model, i.e., proportion of correctly predicted observations. 

<center><img src="/module1/predit_total.gif" > </center>

Sometimes you will also see people reporting error, which is usually  1âˆ’ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ 

<center><img src="/module1/error.gif" > </center>

```{python}
print("The accuracy of the model on the training data: %0.3f" %(dummy_clf.score(X, y)))
```

```{python}
print("The error of the model on the training data: %0.3f" %(1 - dummy_clf.score(X, y)))
```

Notes: 

Its at this point where we can see how well our baseline model predicts the `quiz2` value.  

In ML models, very often it is not possible to get 100% accuracy.
How do you check how well your model is doing?

In the classification setting, `score()` gives the accuracy of the model, i.e., proportion of correctly predicted.

Sometimes you will also see people reporting error, which is usually  1 - accuracy. 

We can see that our model's accuracy on our quiz2 problem is 0.524. 

We could also say the error is 0.476. 


---

## fit and predict paradigms

The general pattern when we build ML models using `sklearn`: 

1. Creating your ğ‘‹ and ğ‘¦ objects 
2. `clf = DummyClassifier()` &rarr; create a model (here we are naming it `clf`)  
3. `clf.fit(X, y)` &rarr; train the model 
4. `clf.score(X, y)` &rarr; assess the model
5. `clf.predict(Xnew)` &rarr; predict on some new data using the trained model 

Notes: 

When building models, there is a general pattern that we repeat. 

1. Creating your ğ‘‹ and ğ‘¦ objects 
2. `clf` &rarr; create a model (here we are naming it `clf`)  
3. `clf.fit(X, y)` &rarr; train the model 
4. `clf.predict(X)` &rarr; predict using the trained model 
5. `clf.score(X, y)` &rarr; assess the model

---

# Letâ€™s apply what we learned!

Notes: <br>

