---
params:
  dynamictitle: "module2_16"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module2/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module2/"
from display_tree import display_tree
```


type: slides

# Decision Tree Regressor

Notes: <br>

---



```{python}
regression_df = pd.read_csv("data/quiz2-grade-toy-regression.csv")
regression_df
```



Notes: 

We saw previously that we can use decision trees for classification problems but we can also use this decision tree algorithm for regression problems.  

Instead of using Gini (which we briefly mentioned this in previous slides), we can use <a href="https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation" target="_blank">some other criteria</a> for splitting. 
(A common one is mean squared error (MSE) which we will discuss shortly)

`scikit-learn` supports regression using decision trees with `DecisionTreeRegressor()` and the `.fit()` and `.predict()` paradigm that is similar to classification.

Just like when we talked about the baseline Dummy regressor, `.score()` for regression returns somethings called an <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" target="_blank"> ùëÖ2 </a>.    

The maximum ùëÖ2 is 1 for perfect predictions. 

It can be negative which is very bad (worse than DummyRegressor). 


--- 

```{python}
X = regression_df.drop(columns=["quiz2"])
X.head()
```

```{python}
y = regression_df["quiz2"]
y.head()
```

Notes: 


Before we do anything, let's bring in our regression data this time it's going to be the quiz2 dataset however instead of predicting quiz2 a categorical variable (A+ or Not A+) we have continuous values instead.

We split our data into our `X` and `Y` objects as we‚Äôve previously been doing.



---

```{python}
from sklearn.tree import DecisionTreeRegressor
```


```{python }
depth = 4
reg_model = DecisionTreeRegressor(max_depth=depth)
reg_model.fit(X, y);
```


Notes: 

**Decision Tree Regressor** is built using `DecisionTreeRegressor()` and a similar `.fit()` and `.predict()` paradigms as classification.

Instead of importing `DecisionTreeClassifier`, we import `DecisionTreeRegressor`.

We follow the same steps as before and can even set hyperparameters as we did in classification. 

Here when we build our model, we are specifying a `max_depth` of 4. 

This means our decision tree is going to be constrained to a depth of 4.



---

```{python include=FALSE, eval = FALSE}
import graphviz
dot_data = export_graphviz(model)
graphviz.Source(
    export_graphviz(
        model4,
        out_file=None,
        feature_names=X.columns,
        class_names=["red", "blue"],
        impurity=True,
    )   
).render("../../../../static/module2/module2_16a", format='png') 
```

```{python include=FALSE, eval = FALSE}
display_tree(X.columns, reg_model, "../../../../static/module2/module2_16a")
```

<center><img src="/module2/module2_16a.png"  width = "68%" alt="404 image" /></center>

    
Notes: 

And here is the tree it produces.

We can see all the decision boundaries and splitting values. 

Our leaves used to contain a categorical value for prediction, but this time we see our leaves are predicting numerical values.



---

```{python}
X.loc[[0]]
```


```{python}
reg_model.predict(X.loc[[0]])
```

Notes: 

Here we take a single example. 

This example has `class_attendance` and `ml_experience` equal to 1 and then the numerical values for labs 1-4 and `quiz2`. 

When we predict on this single example, we can see that our model outputs a value of 90. 


---

```{python}
predicted_grades = reg_model.predict(X)
regression_df = regression_df.assign(predicted_quiz2 = predicted_grades)
print("R^2 score on the training data:" + str(round(reg_model.score(X,y), 3)))
```

```{python}
regression_df.head()
```



Notes: 

Let's see how well this model does predicting on the entire data.

Now we are using `.score()` on the entire data that we‚Äôve trained on. 

We can compare the predicted value versus the true quiz2 grade in this dataframe and we see our model has predicted every example correctly. 

This is confirmed when we see that the score is 1.0. 

We talked in Module 1 about how we use a measurement called ùëÖ2 to measure the score of regression models. An ùëÖ2 score of 1.0, means the model perfectly predicts the outcome of every observation.

This is quite different from what we were getting with a Dummy Classifier which had an ùëÖ2 value of 0.


---

# Let‚Äôs apply what we learned!

Notes: <br>
