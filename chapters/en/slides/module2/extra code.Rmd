
```{python}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
from sklearn.compose import make_column_transformer, ColumnTransformer

# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

```



```{python}
pk = pd.read_csv('data/pokemon.csv')

## Adjusted path 
path = "../../../../static/module3/"
pk['legendary'] =  np.where(pk['legendary'] == 1, "Lengend", "Reg")
pk

X= pk.drop(columns = ['deck_no', 'name', 'type', 'legendary', 'total_bs'])
y = pk['legendary']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=5335)
model = DecisionTreeClassifier(max_depth=4)
model.fit(X_train,y_train)
```

```{python}

display_tree(X_train.columns, model, "../../../../static/module2/Q16")

```
```{python}
X_example = np.array([133, 101, 52, 23, 74, 12, 5]).reshape(1,7)

model.predict(X_example)
```


```{python echo=FALSE}
hockey = pd.read_csv('data/canucks.csv')

## Adjusted path 
path = "../../../../static/module2/"

hockey = hockey[(hockey['Position'] != 'Goalie') ]
hockey
```

```{python}

xmin = hockey['Experience'].min()
xmax = hockey['Experience'].max()
ymin = hockey['Weight'].min()
ymax = hockey['Weight'].max()
model = DecisionTreeClassifier(max_depth=3)
model.fit(hockey.loc[:,['Experience','Weight']], hockey['Position'])
plt.figure(figsize=(4, 4))
plot_classifier(hockey.loc[:,['Experience','Weight']].to_numpy(), hockey['Position'].to_numpy(), model, lims=(0,15,160,230), ticks=True)
plt.ylabel("Player's Weight", fontsize=15)
plt.xlabel("Player's Experience", fontsize=15)
plt.xticks(fontsize= 12)
plt.yticks(fontsize= 12)
plt.savefig('hockey_q.png')
```

```{python}
hockey['Experience'].min()
hockey['Experience'].max()
hockey['Weight'].min()
hockey['Weight'].max()
```

```{python}
from matplotlib import pyplot as plt


```


```{python}

display_tree(hockey.loc[:,['Experience','Weight']].columns, model, "../../../../static/module2/module_hockey2")

```

```{python}
X= pk.drop(columns = ['deck_no', 'name', 'type', 'legendary', 'total_bs'])
y = pk['legendary']
model = DecisionTreeClassifier(max_depth=4)
model.fit(X,y)
display_tree(X.columns, model, "../../../../static/module2/pokemon")

```

```{python}
X_example = np.array([101, 59, 64, 23, 45, 76, 5]).reshape(1,7)

model.predict(X_example)
```

```{python}
pokemon_ob = pd.DataFrame(X_example, columns = X.columns)
pokemon_ob
```


```{python}
df = pd.read_csv("data/canada_usa_cities.csv")
X = df.drop(["country"], axis=1)
y = df["country"]
depth=1
model = DecisionTreeClassifier(max_depth=depth)
model.fit(X,y)
plt.figure(figsize=(4, 4))
plot_classifier(X.to_numpy(), y.to_numpy(), model)
plt.title("Decision tree with depth = %d" % (depth))
```
```{python}
pk
```

```{python}
pk = pd.read_csv('data/pokemon.csv').dropna()
pk
## Adjusted path 
path = "../../../../static/module4/"

X= pk.drop(columns = ['Unnamed: 0', 'song_title', 'song_title','artist', 'target'])
y = pk['target']
X
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=7)
```


```{python}
bball = pd.read_csv('bball.csv')

## Adjusted path 
path = "../../../../static/module4/"

bball['height']= bball['height']*100
bball['weight']= (bball['weight']*2.20462).round(2)
bball = bball[(bball['position'] == 'F') | (bball['position'] == 'G') ]
bball
```


```{python}


```

```{python}

from sklearn.neighbors import KNeighborsClassifier
results_dict = {"n_neighbors": [], "mean_train_score": [], "mean_cv_score": []}


for K in range(1,30):
    knn100 = KNeighborsClassifier(n_neighbors=K)
    scores = cross_validate(knn100, X_train, y_train.to_numpy(),cv=10, return_train_score = True)
    results_dict["n_neighbors"].append(K)
    results_dict["mean_cv_score"].append(np.mean(scores["test_score"]))
    results_dict["mean_train_score"].append(np.mean(scores["train_score"]))

```


```{python}
bball = pd.read_csv('bball.csv')
bball = bball[(bball['position'] == 'F') | (bball['position'] == 'G') ]
bball
```



```{python  fig.width = 12, fig.height = 7,  out.width = '70%', fig.align='center'}

bball = pd.read_csv('bball.csv')
#bball = bball[(bball['position'] == 'F') | (bball['position'] == 'G') ]
bball = bball[(bball['draft_year'] != 'Undrafted') & (bball['draft_round'] != 'Undrafted') & (bball['draft_peak'] != 'Undrafted')]

train_df, test_df = train_test_split(bball, test_size=0.2, random_state=42)

X_train = train_df.drop(columns=['full_name', 'jersey', 'b_day', 'college'])
y_train = train_df['position']

X_test = test_df.drop(columns=['full_name', 'jersey', 'b_day', 'college'])
y_test = test_df['position']

numeric_features = [
    "rating",
    "height",
    "weight",
    "salary",
    "draft_year",
    "draft_round",
    "draft_peak"
]

categorical_features = [
    "team",
    "country"
]

numeric_transformer = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"),
)

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features)
)

pipe = make_pipeline(preprocessor, SVC())
pipe.fit(X_train, y_train);

plot_confusion_matrix(pipe, X_test, y_test, values_format="d", cmap="Blues", labels = ['F', 'C', 'G', 'G-F', 'F-C', 'F-G']);
plt.show()
```
```{python}
from sklearn.metrics import classification_report


y_test.unique()

```



```{python}
source = results_df.melt(id_vars=['n_neighbors'] , 
                              value_vars=['mean_train_score', 'mean_cv_score'], 
                              var_name='plot', value_name='score')
source
```


```{python}
chart1 = alt.Chart(source).mark_line().encode(
    alt.X('n_neighbors:Q', axis=alt.Axis(title="n_neighbors")),
    alt.Y('score:Q', scale=alt.Scale(domain=[.82, 1.00])),
    alt.Color('plot:N', scale=alt.Scale(domain=['mean_train_score', 'mean_cv_score'],
                                           range=['teal', 'gold'])))
chart1
```

```{python include =FALSE}
chart1.save(path + 'Q18a.png')
```


```{python}
bball = pd.read_csv('data/basketball.csv')
bball['height'].str.split('/ ', expand=True)[[0]]
bball['weight'].str.split('/ ', expand=True)[0].str.strip(' lbs.')
bball['height'] = bball['height'].str.split('/ ', expand=True)[[1]].astype('float')
bball['weight'] = bball['weight'].str.split('/ ', expand=True)[1].str.strip(' kg.').astype('float')
bball['salary'] = bball['salary'].str.strip('$').astype('float')
bball.to_csv('bball.csv', index=False)
bball
```


```{python}
bball['position'].value_counts() 
```


```{python}
# Loading in the data
pokemon_df = pd.read_csv('data/pokemon.csv')

# Define X and y
X = pokemon_df.drop(columns = ['deck_no', 'name','total_bs', 'type', 'legendary'])
y = pokemon_df['legendary']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=33)

results_dict = {"n_neighbors": [], "mean_train_score": [], "mean_cv_score": []}

# Create a for loop and fill in the blanks
for k in range(1,50):
    model = NearestNeighbors(n_neighbors=k)
    scores = cross_validate(model, X_train, y_train, return_train_score=True)
    results_dict["n_neighbors"].append(k)
    results_dict["mean_cv_score"].append(scores["test_score"].mean())
    results_dict["mean_train_score"].append(scores["train_score"].mean())

# Wrangles the data into a form suitable for plotting 
results_df = pd.DataFrame(results_dict).melt(id_vars=['n_neighbors'],
                                             value_vars=['mean_train_score',
                                                         'mean_cv_score'], 
                                             var_name='split',
                                             value_name='score')

# Create a chart that plots depth vs score
chart1 = alt.Chart(results_df).mark_line().encode(
         alt.X('n_neighbors:Q', axis=alt.Axis(title="Number of Neighbours")),
         alt.Y('score:Q', scale=alt.Scale(domain=[.80, 1.00])), 
         alt.Color('split:N', scale=alt.Scale(domain=['mean_train_score',
                                                     'mean_cv_score'],
                                             range=['teal', 'gold'])))
chart1
```

```{python}
cities_df = pd.read_csv("data/canada_usa_cities.csv")
train_df, test_df = train_test_split(cities_df, test_size=0.4, random_state=77)
train_df.head()
X_train, y_train = train_df.drop(columns=['country']), train_df['country']
X_test, y_test = test_df.drop(columns=['country']), test_df['country']
```
```{python}
pk
```

```{python}
pk = pk.sample(100)
X= pk[['attack', 'defense']]
y = pk['legendary']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=77)
```


```{python }
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
scores = cross_validate(knn, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```

```{python}
svm = SVC(gamma=0.001, C=100)
svm.fit(X_train, y_train)
scores = cross_validate(svm, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```

```{python echo=FALSE}
plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
#plt.title("")
plot_classifier(X_train, y_train, knn, ax=plt.gca())
plt.subplot(1, 2, 2)
plt.title("")
plot_classifier(X_train, y_train, svm, ax=plt.gca())

```



```{python}
nb_file = tempfile(fileext = '.ipynb')
jsonlite::write_json(nb_data, nb_file, auto_unbox = TRUE, pretty = TRUE)
xfun::file_string(nb_file)  # show file content

# convert to R Markdown
nb_rmd = rmarkdown:::convert_ipynb(nb_file)
```

```{python}
u = np.array([5, 0, 22, -11])
v = np.array([-1, 0, 19, -9])
w = np.array([0, 1, 17, -4])
```

```{python}
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
euclidean_distances([v,w])
```
```{python}
data = [[ 'apple', 'red', 'canada', True, 'round', True, 84, 100],
        ['banana', 'yellow', 'mexico', False, 'long', True, 75, 120],
        ['cantaloupe', 'orange', 'spain', True, 'round', True, 90, 1360],
        ['dragon-fruit', 'magenta', 'china', True, 'round', False, 96, 600],
        ['elderberry', 'purple', 'austria', False, 'round', True, 80, 5],
        ['fig', 'purple', 'turkey', False, 'oval', False, 78, 40],
        ['guava', 'green', 'mexico', True, 'oval', True, 83, 450],
        ['huckleberry', 'blue', 'canada', True, 'round', True, 73, 5],
        ['kiwi', 'brown', 'china', True, 'round', True, 80, 76],
        ['lemon', 'yellow', 'mexico', False, 'oval', False, 83, 65],
       ] 
  
# Create the pandas DataFrame 
fruit_salad = pd.DataFrame(data, columns = ['name', 'colour', 'location', 'seed', 'shape', 'sweetness', 'water_content', 'weight' ]) 
fruit_salad
```

```{python}
data = [[ 'apple', 'red', 'canada', True, 'round', True, 84, 100],
        ['banana', 'yellow', 'mexico', False, 'long', True, 75, 120],
        ['cantaloupe', 'orange', 'spain', None, 'round', True, 90, 1360],
        ['dragon-fruit', 'magenta', None, True, 'round', False, 96, 600],
        ['elderberry', 'purple', 'austria', False, 'round', True, 80, 5],
        ['fig', 'purple', 'turkey', False, 'oval', False, 78, 40],
        ['guava', 'green', 'mexico', True, 'oval', True, 83, 450],
        ['huckleberry', 'blue', 'canada', True, 'round', True, 73, 5],
        ['kiwi', 'brown', 'china', True, 'round', True, 80, 76],
        ['lemon', 'yellow', 'mexico', False, 'oval', False, 83, 65],
       ] 
  
# Create the pandas DataFrame 
fruit_salad = pd.DataFrame(data, columns = ['name', 'colour', 'location', 'seed', 'shape', 'sweetness', 'water_content', 'weight' ]) 
fruit_salad
```


```{python}
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse=False, dtype='int')
ohe.fit(fruit_salad[['shape']]);
X_toy_ohe = ohe.transform(fruit_salad[['shape']])

X_toy_ohe.T
```
```{python}
from sklearn.preprocessing import OrdinalEncoder

oe = OrdinalEncoder(dtype=int)
oe.fit(X_toy);
X_toy_ord = oe.transform(X_toy)

X_toy_ord
```


```{python}
pd.DataFrame(
    data=X_toy_ohe,
    columns=ohe.get_feature_names(['shape']),
    index=fruit_salad.index,
)
```


```{python}
data = [[ 'apple', 'red',False, 'canada', 6, True, 'round', True, 84, 100],
        ['banana', 'yellow', True, 'mexico',12, False, 'long', True, 75, 120],
        ['cantaloupe', 'orange', False, 'spain',8, True, 'round', True, 90, 1360],
        ['dragon-fruit', 'magenta', False, 'china',18, True, 'round', False, 96, 600],
        ['elderberry', 'purple',False, 'austria',11, False, 'round', True, 80, 5],
        ['fig', 'purple', False,  'turkey',8, False, 'oval', False, 78, 40],
        ['guava', 'green', True, 'mexico',14, True, 'oval', True, 83, 450],
        ['huckleberry', 'blue', False,  'canada',6, True, 'round', True, 73, 5],
        ['kiwi', 'brown', True, 'china',8, True, 'round', True, 80, 76],
        ['lemon', 'yellow', True, 'mexico',4, False, 'oval', False, 83, 65],
       ] 
  
# Create the pandas DataFrame 
fruit_salad = pd.DataFrame(data, columns = ['name', 'colour','tropical', 'location','carbs', 'seed', 'shape', 'sweetness', 'water_content', 'weight' ]) 
fruit_salad
```

```{python}
fruit_salad.drop(columns = ['name'])

numeric_features = ['water_content', 'weight', 'carbs']

categorical_features = ['colour', 'location', 'seed', 'shape', 'sweetness', 'tropical']
```

```{python}
numeric_transformer = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="constant", fill_value="missing"),
    OneHotEncoder(handle_unknown="ignore"),
)

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features)
)

pipe = make_pipeline(preprocessor, DecisionTreeClassifier())

preprocessor
```

```{python}
from sklearn import set_config
set_config(display='diagram')
pipe
```

```{python}
X = [ "Take me to the river",
    "Drop me in the water.",
    "Push me in the river,",
    " dip me in the water"]

```


```{python}
from sklearn.feature_extraction.text import CountVectorizer
```

```{python}
vec = CountVectorizer()
X_counts = vec.fit_transform(X);
bow_df = pd.DataFrame(X_counts.toarray(), columns=sorted(vec.vocabulary_), index=X)
bow_df
```


```{python}
len(vec.vocabulary_)
```

```{python}
vec8_binary = CountVectorizer(binary=True, max_features=5)
X_counts_binary = vec8_binary.fit_transform(X)
pd.DataFrame(data = X_counts_binary.sum(axis=0).tolist()[0], 
             index = vec8_binary.get_feature_names(), columns=['counts']).sort_values('counts', ascending=False)
X_counts_binary.toarray()
```

```{python}
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Loading in the data
pk_df = pd.read_csv('data/pokemon.csv')

train_df, test_df = train_test_split(pk_df, test_size=0.2, random_state=1)

X_train_big = train_df.drop(columns=['legendary'])
y_train_big = train_df['legendary']
X_test = test_df.drop(columns=['legendary'])
y_test = test_df['legendary']

X_train, X_valid, y_train, y_valid = train_test_split(X_train_big, 
                                                      y_train_big, 
                                                      test_size=0.3, 
                                                      random_state=123)

numeric_features = ["deck_no",  
                    "attack",
                    "defense" ,
                    "sp_attack",
                    "sp_defense",
                    "speed",
                    "capture_rt",
                    "total_bs"]

categorical_features = [
    "type"]

numeric_transformer = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"),
)

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features)
)

# Build a pipeline containing the column transformer and an SVC model
# Name this pipeline pipe_unbalanced
pipe_unbalanced = make_pipeline(preprocessor, SVC())

# Fit your unbalanced pipeline on the training data
pipe_unbalanced.fit(X_train, y_train);

# Predict your values on the validation set
# Save them in an object named unbalanced_predicted
unbalanced_predicted = pipe_unbalanced.predict(X_valid)

# Using sklearn tools, print a classification_report from the validation set
print(classification_report(y_valid, unbalanced_predicted, digits=5, zero_division=0))


# Build another pipeline containing the column transformer and an SVC model
# This time use the parameter class_weight="balanced"
# Name this pipeline pipe_balanced
pipe_balanced = make_pipeline(preprocessor, SVC(class_weight="balanced"))

# Fit your balanced pipeline on the training data
pipe_balanced.fit(X_train, y_train);

# Predict your values on the validation set
# Save them in an object named balanced_predicted
balanced_predicted = pipe_balanced.predict(X_valid)

# Using sklearn tools, print a classification_report from the validation set
print(classification_report(y_valid, balanced_predicted, digits=5, zero_division=0))
```

```{python}
pk_df
```


```{python}
from sklearn.metrics import mean_squared_error 

true =    [4, 12, 6, 9, 3,]
predict = [5, 10, 9, 8, 3,]

mean_squared_error( true, predict)
tru

```
```{python}
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.svm import SVC

# Loading in the data
pk_df = pd.read_csv('data/pokemon.csv')

train_df, test_df = train_test_split(pk_df, test_size=0.2, random_state=1)

X_train = train_df.drop(columns=['legendary'])
y_train = train_df['legendary']
X_test = test_df.drop(columns=['legendary'])
y_test = test_df['legendary']

numeric_features = ["deck_no",  
                    "attack",
                    "defense" ,
                    "sp_attack",
                    "sp_defense",
                    "speed",
                    "capture_rt",
                    "total_bs"]

categorical_features = [
    "type"]

numeric_transformer = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"),
)

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features)
)

#param_grid = {"svc__gamma": [1, 10, 100],
#              "svc__C": [1, 10, 100]}


# Build a pipeline containing the column transformer and an SVC model
# Use the parameter class_weight="balanced"
# Name this pipeline main_pipe
main_pipe = make_pipeline(preprocessor, SVC(class_weight="balanced"))

# Perform cross validation on the training split using the scoring measures `accuracy, precision and recall 
# You do not need to include return_train_score=True for this
# Save the results in a dataframe named multi_scores
multi_scores = pd.DataFrame(cross_validate(main_pipe,  X_train, y_train, scoring = ['accuracy','precision', 'recall']))
print(multi_scores)

```

```{python}
import numpy as np
import pandas as pd
import scipy
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import  mean_squared_error, r2_score
from scipy.stats import randint
from sklearn.metrics import make_scorer

# Loading in the data
bball = pd.read_csv('data/bball.csv')
bball = bball[(bball['draft_year'] != 'Undrafted') & (bball['draft_round'] != 'Undrafted') & (bball['draft_peak'] != 'Undrafted')]

train_df, test_df = train_test_split(bball, test_size=0.2, random_state=1)

X_train = train_df[['height']]
y_train = train_df['weight']
X_test = test_df[['height']]
y_test = test_df['weight']

## Define mape function 
def mape(true, pred):
    return 100.*np.mean(np.abs((pred - true) / true))

## Create a mape scorer where lower number are better 
neg_mape_scorer = make_scorer(mape, greater_is_better=False)

# Create a set of values for alpha
param_dist = {
    "alpha": scipy.stats.randint(low=1, high=10000)}

# Build a Ridge model called ridge_bb
ridge_bb = Ridge()

## Use RandomizedSearchCV to hyperparameter tune. 
random_search = RandomizedSearchCV(
    ridge_bb, param_dist, n_iter=20,
    cv=5, verbose=1, n_jobs=1,
    scoring=neg_mape_scorer, random_state=123)

# Fit your grid search on the training data
random_search.fit(X_train, y_train)

# What is the best value for alpha?
# Save it in an object named best_alpha
best_alpha = random_search.best_params_
print(best_alpha)

# What is the best MAPE score?
# Save it in an object named best_mape
best_mape = random_search.best_score_
print(best_mape)
```


```{python}
import numpy as np
import pandas as pd
import scipy
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.linear_model import Ridge
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.metrics import  mean_squared_error, r2_score
from scipy.stats import randint
from sklearn.metrics import make_scorer

# Loading in the data
bball = pd.read_csv('data/bball.csv')
bball = bball[(bball['draft_year'] != 'Undrafted') & (bball['draft_round'] != 'Undrafted') & (bball['draft_peak'] != 'Undrafted')]
bball = bball.replace({'F-G': 'Other', 'F-C': 'Other', 'G-F': 'Other', 'C-F': 'Other', 'C': 'Other'})
df_train, df_test = train_test_split(bball, test_size=0.2, random_state=1)


X_train = df_train[["weight", "height", "draft_year", "draft_round",
                     "draft_peak", "team", "salary", "country"]]
X_test = df_test[["weight", "height", "draft_year", "draft_round",
                     "draft_peak", "team", "salary", "country"]]
y_train = df_train['position']
y_test = df_test['position']


# Split the numeric and categorical features 
numeric_features = [ "weight",
                     "height",
                     "draft_year",
                     "draft_round",
                     "draft_peak"]

passthrough_features = ["team", "country"]


def mape(true, pred):
    return 100.*np.mean(np.abs((pred - true) / true))
neg_mape_scorer = make_scorer(mape, greater_is_better=False)

# Build a numeric pipeline
numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"))

# Build a categorical pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# Build a categorical transformer
col_transformer = make_column_transformer(
    
    (numeric_transformer, numeric_features),
    ('drop', categorical_features))

# Build a main pipeline
ridge_bb = make_pipeline(
    col_transformer,
    LogisticRegression())
    


#param_dist = {
#    "ridge__alpha": scipy.stats.randint(low=1, high=10000)}

#random_search = RandomizedSearchCV(
#    ridge_bb, param_dist,
#    n_iter=20, cv=5, verbose=1, n_jobs=1, random_state=123, scoring= neg_mape_scorer)

# Fit your grid search on the training data
ridge_bb.fit(X_train, y_train)

ridge_bb['logisticregression'].coef_
```

```{python}
X_train.columns
```

```{python}
pd.DataFrame(data=ridge_bb['logisticregression'].coef_.T, index=numeric_features, columns=ridge_bb['logisticregression'].classes_)
```



```{python}
import numpy as np
import pandas as pd
import scipy
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import  mean_squared_error, r2_score
from scipy.stats import randint
from sklearn.metrics import make_scorer

# Loading in the data
bball = pd.read_csv('data/bball.csv')
bball = bball[(bball['draft_year'] != 'Undrafted') & (bball['draft_round'] != 'Undrafted') & (bball['draft_peak'] != 'Undrafted')]
bball = bball.replace({'F-G': 'Other', 'F-C': 'Other', 'G-F': 'Other', 'C-F': 'Other', 'C': 'Other'})

train_df, test_df = train_test_split(bball, test_size=0.2, random_state=1)

X_train = train_df[['height', 'weight']]
y_train = train_df['salary']
X_test = test_df[['height', 'weight']]
y_test = test_df['salary']

player_stats = pd.DataFrame([[2.05,93.2]], columns =['height', 'weight'])
player_stats

# Build a Ridge model called ridge_bb
ridge_bb = Ridge()

# Fit your grid search on the training data
ridge_bb.fit(X_train, y_train)

# What are the coefficients for this model?
# Save these in an object named bb_weights

bb_weights = ridge_bb.coef_
print(bb_weights)

# What is the intercept for this model? 
# Save this in an object named bb_intercept

bb_intercept = ridge_bb.intercept_
print(bb_intercept)

# Using the weights and intercept discovered above, 
# calculate the models's prediction
# Save it in an object named player_predict

player_predict = bb_intercept + (bb_weights*player_stats).sum(axis=1)
print(player_predict)

# Check your answer using predict
ridge_bb.predict(player_stats)

```


```{python}
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression

# Loading in the data
pk_df = pd.read_csv('data/pokemon.csv')

train_df, test_df = train_test_split(pk_df, test_size=0.2, random_state=1)

X_train = train_df.drop(columns=['legendary'])
y_train = train_df['legendary']
X_test = test_df.drop(columns=['legendary'])
y_test = test_df['legendary']


numeric_features = ["attack",
                    "defense" ,
                    "sp_attack",
                    "sp_defense",
                    "speed",
                    "capture_rt",
                    "total_bs"]

drop_features = ["type", "deck_no", "gen", "name"]

numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler())


categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"),
)

preprocessor = make_column_transformer(
    ("drop", drop_features),
    (numeric_transformer, numeric_features)
)

# Build a pipeline containing the column transformer and an Logistic Regression model
# use the parameter class_weight="balanced"
# Name this pipeline pkm_pipe
pkm_pipe = make_pipeline(preprocessor, LogisticRegression(class_weight="balanced"))


# Fit your pipeline on the training data
pkm_pipe.fit(X_train, y_train);


# Score your model on the test set 
# Save them in an object named lr_scores
lr_scores = pkm_pipe.score(X_test, y_test)
print("logistic Regression Test Score:", lr_scores)

# Fill in the blanks below to asses the model's feature coefficients. 

data = pd.DataFrame({'features':numeric_features, 'coefficients':pkm_pipe['logisticregression'].coef_[0]})
data
```


```{python}
pkm_pipe['logisticregression'].coef_[0].shape

pkm_pipe['columntransformer'].transformers_[2]['onehot']\
                   .get_feature_names(categorical_features)
```
```{python}
pkm_pipe['logisticregression'].coef_[0]
```

```{python}
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Loading in the data
pk_df = pd.read_csv('data/pokemon.csv')

train_df, test_df = train_test_split(pk_df, test_size=0.2, random_state=1)

X_train_big = train_df.drop(columns=['legendary'])
y_train_big = train_df['legendary']
X_test = test_df.drop(columns=['legendary'])
y_test = test_df['legendary']

X_train, X_valid, y_train, y_valid = train_test_split(X_train_big, 
                                                      y_train_big, 
                                                      test_size=0.3, 
                                                      random_state=123)

numeric_features = ["deck_no",  
                    "attack",
                    "defense" ,
                    "sp_attack",
                    "sp_defense",
                    "speed",
                    "capture_rt",
                    "total_bs"]

categorical_features = ["type"]

numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"),
)

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features)
)

# Build a pipeline containing the column transformer and an SVC model
# Name this pipeline pipe_unbalanced
pipe_unbalanced = make_pipeline(preprocessor, SVC())

# Fit your unbalanced pipeline on the training data
pipe_unbalanced.fit(X_train, y_train);

# Predict your values on the validation set
# Save them in an object named unbalanced_predicted
unbalanced_predicted = pipe_unbalanced.predict(X_valid)

# Using sklearn tools, print a classification_report from the validation set
print(classification_report(y_valid, unbalanced_predicted, digits=2))

# Build another pipeline containing the column transformer and an SVC model
# This time use the parameter class_weight="balanced"
# Name this pipeline pipe_balanced
pipe_balanced = make_pipeline(preprocessor, SVC(class_weight="balanced"))

# Fit your balanced pipeline on the training data
pipe_balanced.fit(X_train, y_train);

# Predict your values on the validation set
# Save them in an object named balanced_predicted
balanced_predicted = pipe_balanced.predict(X_valid)

# Using sklearn tools, print a classification_report from the validation set
print(classification_report(y_valid, balanced_predicted, digits=2))
```

```{python}
import numpy as np
import pandas as pd
import scipy
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Loading in the data
pk_df = pd.read_csv('data/pokemon.csv')

train_df, test_df = train_test_split(pk_df, test_size=0.2, random_state=1)

X_train_big = train_df.drop(columns=['legendary'])
y_train_big = train_df['legendary']
X_test = test_df.drop(columns=['legendary'])
y_test = test_df['legendary']

X_train, X_valid, y_train, y_valid = train_test_split(X_train_big, 
                                                      y_train_big, 
                                                      test_size=0.3, 
                                                      random_state=123)

numeric_features = ["deck_no",  
                    "attack",
                    "defense" ,
                    "sp_attack",
                    "sp_defense",
                    "speed",
                    "capture_rt",
                    "total_bs"]

categorical_features = ["type"]

numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"), 
    StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features))
    
param_grid = {"logisticregression__C": scipy.stats.uniform(0, 100)}

# Build a pipeline containing the column transformer and an Logistic Regression model
# use the parameter class_weight="balanced" and set 
# Name this pipeline pkm_pipe
pkm_pipe = make_pipeline(preprocessor, LogisticRegression(class_weight="balanced"))

# perform RandomizedSearchCV using the parameters specified in param_grid
# Use n_iter equal to 10, 5 cross-validation folds and return the training score. 
# Name this object pkm_grid
pkm_grid = RandomizedSearchCV(pkm_pipe, param_grid, n_jobs=-1, cv=5, return_train_score=True, n_iter=10, scoring = 'f1', random_state=2021)

#Train your pmk_grid on the training data
pkm_grid.fit(X_train, y_train)

## What is the best max_features value? Save it in an object name tweet_feats
pkm_best_c= pkm_grid.best_params_['logisticregression__C']
print(pkm_best_c)

## What is the best score? Save it in an object named tweet_val_score
pkm_best_score = pkm_grid.best_score_
print(pkm_best_score)
```

```{python}
import numpy as np
import pandas as pd
import scipy
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Loading in the data
pk_df = pd.read_csv('data/pokemon.csv')

train_df, test_df = train_test_split(pk_df, test_size=0.2, random_state=1)

X_train_big = train_df.drop(columns=['legendary'])
y_train_big = train_df['legendary']
X_test = test_df.drop(columns=['legendary'])
y_test = test_df['legendary']

X_train, X_valid, y_train, y_valid = train_test_split(X_train_big, 
                                                      y_train_big, 
                                                      test_size=0.3, 
                                                      random_state=123)

numeric_features = ["deck_no",  
                    "attack",
                    "defense" ,
                    "sp_attack",
                    "sp_defense",
                    "speed",
                    "capture_rt",
                    "total_bs"]

categorical_features = ["type"]

numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"), 
    StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features))
    
param_grid = {"logisticregression__C": scipy.stats.uniform(0, 100)}

# Build a pipeline containing the column transformer and an Logistic Regression model
# use the parameter class_weight="balanced" and set 
# Name this pipeline pkm_pipe
pkm_pipe = make_pipeline(preprocessor, LogisticRegression(class_weight="balanced"))

# perform RandomizedSearchCV using the parameters specified in param_grid
# Use n_iter equal to 10, 5 cross-validation folds and return the training score. 
# Name this object pkm_grid
pkm_grid = RandomizedSearchCV(pkm_pipe, param_grid, n_jobs=-1, cv=5, return_train_score=True, n_iter=10, scoring = 'f1', random_state=2028)

# Train your pmk_grid on the training data
pkm_grid.fit(X_train, y_train)

# What is the best C value? Save it in an object name tweet_feats
pkm_best_c= pkm_grid.best_params_['logisticregression__C']
print("Best C value:", pkm_best_c)

# What is the best score? Save it in an object named tweet_val_score
pkm_best_score = pkm_grid.best_score_
print("Best f1 score:", pkm_best_score)

# Find the predictions of the test set using predict. 
# Save this in an object named predicted_y
predicted_y = pkm_grid.predict(X_test)

# Find the target class probabilities of the test set using predict_proba. 
# Save this in an object named proba_y
proba_y = pkm_grid.predict_proba(X_test)

# This next part has been done for you. 
lr_probs = pd.DataFrame({
             "true y":y_test, 
             "pred y": predicted_y.tolist(),
             "prob_legend": proba_y[:, 1].tolist()})
             
# Take the dataframe lr_probs and sort them in descending order of the models confidence
# in predicting legendary pokemon
# Save this in an object named legend_sorted       
legend_sorted = lr_probs.sort_values(by='prob_legend', ascending=False)
legend_sorted
```

```{python}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import plot_confusion_matrix


# Loading in the data
bball = pd.read_csv('data/bball.csv')
bball = bball[(bball['draft_year'] != 'Undrafted') & (bball['draft_round'] != 'Undrafted') & (bball['draft_peak'] != 'Undrafted')]
bball = bball.replace({'F-G': 'Other', 'F-C': 'Other', 'G-F': 'Other', 'C-F': 'Other', 'C': 'Other'})
df_train, df_test = train_test_split(bball, test_size=0.2, random_state=1)


X_train = df_train[["weight", "height", "draft_year", "draft_round",
                     "draft_peak", "team", "salary", "country"]]
X_test = df_test[["weight", "height", "draft_year", "draft_round",
                     "draft_peak", "team", "salary", "country"]]
y_train = df_train['position']
y_test = df_test['position']


# Split the numeric and categorical features 
numeric_features = [ "weight",
                     "height",
                     "draft_year",
                     "draft_round",
                     "draft_peak"]

categorical_features = ["team", "country"]


# Build a numeric pipeline
numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"))

# Build a categorical pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))


# Build a numeric pipeline
numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler())

# Build a categorical pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# Build a categorical transformer
col_transformer = make_column_transformer(
    (numeric_transformer, numeric_features),
    (categorical_transformer, categorical_features))

# Build a main pipeline
lr_pipe = make_pipeline(
    col_transformer,
    LogisticRegression())

# Fit your pipeline on the training set
lr_pipe.fit( X_train, y_train)

# Plot your confusion matrix on your test set 
plot_confusion_matrix(lr_pipe, X_test, y_test,
                      cmap="PuRd")
plt.show()
                  
```
```{python}
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
```

```{python}
# Predict your values on the validation set
# Save them in an object named balanced_predicted
balanced_predicted = lr_pipe.predict(X_test)

# Using sklearn tools, print a classification_report from the validation set
print(classification_report(y_test, balanced_predicted, digits=2))
```

```{python}
import numpy as np
import pandas as pd
import scipy
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Loading in the data
pk_df = pd.read_csv('data/pokemon.csv')

train_df, test_df = train_test_split(pk_df, test_size=0.2, random_state=1)

X_train_big = train_df.drop(columns=['legendary'])
y_train_big = train_df['legendary']
X_test = test_df.drop(columns=['legendary'])
y_test = test_df['legendary']

X_train, X_valid, y_train, y_valid = train_test_split(X_train_big, 
                                                      y_train_big, 
                                                      test_size=0.3, 
                                                      random_state=123)

numeric_features = ["deck_no",  
                    "attack",
                    "defense" ,
                    "sp_attack",
                    "sp_defense",
                    "speed",
                    "capture_rt",
                    "total_bs"]

categorical_features = ["type"]

numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median"), 
    StandardScaler())

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features), 
    (categorical_transformer, categorical_features))
    
param_grid = {"logisticregression__C": scipy.stats.uniform(0, 100)}

# Build a pipeline containing the column transformer and a Logistic Regression model
# use the parameter class_weight="balanced" and set max_iter=1000
# Name this pipeline pkm_pipe
pkm_pipe = make_pipeline(preprocessor, LogisticRegression(class_weight="balanced", max_iter=1000))

# Perform RandomizedSearchCV using the parameters specified in param_grid
# Use n_iter equal to 10, 5 cross-validation folds and return the training score. 
# Name this object pmk_search
pmk_search = RandomizedSearchCV(pkm_pipe, param_grid, n_jobs=-1, cv=5, return_train_score=True, n_iter=10, scoring = 'f1', random_state=2028)

# Train your pmk_search on the training data
pmk_search.fit(X_train, y_train)

# What is the best C value? Save it in an object name pkm_best_c
pkm_best_c= pmk_search.best_params_['logisticregression__C']
print("Best C value:", pkm_best_c)

# What is the best f1 score? Save it in an object named pkm_best_score
pkm_best_score = pmk_search.best_score_
print("Best f1 score:", pkm_best_score)

# Find the predictions of the test set using predict. 
# Save this in an object named predicted_y
predicted_y = pmk_search.predict(X_test)

# Find the target class probabilities of the test set using predict_proba. 
# Save this in an object named proba_y
proba_y = pmk_search.predict_proba(X_test)

# This next part has been done for you
lr_probs = pd.DataFrame({
             "Pokemon": test_df['name'],
             "true y":y_test, 
             "pred y": predicted_y.tolist(),
             "prob_legend": proba_y[:, 1].tolist()})
             
# Take the dataframe lr_probs and sort them in descending order of the models confidence
# in predicting legendary pokemon
# Save this in an object named legend_sorted       
legend_sorted = lr_probs.sort_values(by='prob_legend', ascending=False)
legend_sorted
```
```{python}
legend_sorted[10:20]
```

```{python}
test_df.loc[[248]]
```
```{python}
pmk_search.classes_
```

