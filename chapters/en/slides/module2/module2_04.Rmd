---
params:
  dynamictitle: "module2_04"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module2/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

```


type: slides

# Decision Tree Classifiers 

Notes: <br>

---

<center>
<img src="/module2/lingo_tree.png"  width = "100%" alt="404 image">
</center>

Notes: 

Now that we know the structure of a decision tree, let's build a decision tree model.  

--- 


```{python}
from sklearn.tree import DecisionTreeClassifier
```


```{python}
model = DecisionTreeClassifier()
```

```{python include=FALSE}
new_ex = [1, 0, 1, 1, 0, 0, 0]
new_example = pd.DataFrame(data= [new_ex], columns = ["ml_experience",  "class_attendance",  "lab1", "lab2", "lab3", "lab4", "quiz1"])
```

```{python}
new_example
```


```python
model.predict(new_example)
```

```out
NotFittedError: This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

Detailed traceback: 
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.8/site-packages/sklearn/tree/_classes.py", line 426, in predict
    check_is_fitted(self)
  File "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py", line 72, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py", line 1019, in check_is_fitted
    raise NotFittedError(msg % {'name': type(estimator).__name__})
```

Notes: 

We can use the same steps we used when building a baseline model, to build a decision tree model with `scikit-learn`.

Recall that `scikit-learn` uses the term `fit` for training or learning and uses `predict` for prediction. 

We must import the function from the `sklearn.tree` library and then we can create a decision tree using the `DecisionTreeClassifier()` function. 

Is our new model ready to predict now that we have made our decision tree model? 

Let's see what happens when we try to predict the quiz2 mark of a new example. 

We get an error! We forgot the crucial step of fitting our model. 


---

```{python include=FALSE}
classification_df = pd.read_csv("data/quiz2-grade-toy-classification.csv")
X = classification_df.drop(["quiz2"], axis=1)
y = classification_df["quiz2"]
```

```{python include=FALSE}
X_binary = X.copy()
columns = ["lab1", "lab2", "lab3", "lab4", "quiz1"]
for col in columns:
    X_binary[col] = X_binary[col].apply(
        lambda x: 1 if x >= 90 else 0)
X_binary.head()    
```

```{python}
X_binary.head()
```
```{python}
y.head()
```


```{python results='hide'}
model.fit(X_binary, y)
```


Notes: 

We need to make sure that we `.fit()` our model before we `.predict()`. 

In the decision tree algorithm, the fitting stage is where the model learns about the data and sets the *if and else* statements.

---


```{python include =FALSE}
import graphviz

graphviz.Source(
    export_graphviz(
        model,
        out_file=None,
        filled = True,
        feature_names=X.columns,
        class_names=["not A+", "A+"],
        rounded = True
    )
).render("../../../../static/module2/dt_quiz2", format='png') 
```

<center><img src="/module2/dt_quiz2.png"  width = "60%" alt="404 image" /></center>

Notes: 

Let's take a look at what our tree looks like. We can see all the nodes of the tree and what the root condition is. 

---

```{python}
new_example
```


```{python}
print("Prediction for example:" + (model.predict(new_example)[0]))
```


Notes: 

Now that the model is fitted, we will be able to predict using the built model.

---


```{python}
model.score(X_binary, y)
```




Notes: 

We can check how accurate our model is using `.score()`.

This model predicts on data it's already seen with 90% accuracy. 

---



### How does predict work? 

```{python include=FALSE}
new_ex = [1, 0, 1, 1, 0, 1, 0]
observation = pd.DataFrame(data= [new_ex], columns = ["ml_experience",  "class_attendance",  "lab1", "lab2", "lab3", "lab4", "quiz1"])
```

```{python echo=TRUE}
observation
```

<center><img src="/module2/predict.gif"  width = "90%" alt="404 image" /></center>

Notes: 

Let's discuss how predict works. 

We have a learned tree and a test example. 

Let's start at the top of the tree and ask binary questions at each node and follow the appropriate path in the tree.

Once you are at a leaf node, you have the prediction. 
 
The model only considers the features which are in the learned tree and ignores all other features. 

---

### How does fit work 

- Which features are most useful for classification? 
- Minimize **impurity** at each question
- Common criteria to minimize impurity 
    - Gini Index
    - Information gain 
    - Cross entropy 


Notes: 

We aren't going to go into this in much detail, but the fitting of a decision tree has a lot to do with important features (which columns contribute the most to the decision-making process) and minimizing the "impurity". 

We don't need to worry about this for this course. 
 
---

# Letâ€™s apply what we learned!

Notes: <br>
