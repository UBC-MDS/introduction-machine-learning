---
params:
  dynamictitle: "module3_17"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module3/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import train_test_split, cross_validate 
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module3/"
from display_tree import display_tree
```


type: slides

# The fundamental tradeoff and the golden rule

Notes: <br>

---

## Reminder: 

- **ùê∏_train**:  is our training error (or mean train error from cross-validation).
- **ùê∏_valid**  is our validation error (or mean validation error from cross-validation).
- **ùê∏_test**  is our test error.
- **ùê∏_best**  is the best possible error we could get for a given problem.

Notes:

Before going further, let's just remind ourselves of the different possible errors. 

---

## The "fundamental tradeoff" of supervised learning

<br>
<br>
<br>

### As model complexity ‚Üë,     ùê∏_train ‚Üì     but ùê∏_valid‚àíùê∏_train  tend to ‚Üë.


Notes: 

If your model is very simple, like `DummyClassifier()`, then you won't really learn any "specific patterns" of the training set, but your model won't be very good in general.

This is **underfitting**.

If your model is very complex, like a `DecisionTreeClassifier(max_depth=None)`, then you will learn unreliable patterns that get every single training example correct, but there will be a huge gap between training error and validation error.

This is **overfitting**.




---

## Bias vs variance tradeoff

<center><img src="/module3/darts_overfitting.png"  width = "50%" alt="404 image" /></center>
 
<a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank">**Attribution: University of Washington - Pedro Domingos**</a>

Notes: 

This fundamental trade-off is also called the ***bias/variance tradeoff***.

**Bias**  ‚Üí  the tendency to consistently learn the same wrong thing (high bias means underfitting).

**Variance**  ‚Üí  the tendency to learn random things irrespective of the real signal (high variance means overfitting).

---

## How to pick a model that would generalize better?


```{python}
df = pd.read_csv("data/canada_usa_cities.csv")
X = df.drop(["country"], axis=1)
y = df["country"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123)
```




Notes: 

So how do we deal with this? 

How do we avoid both underfitting and overfitting?

First, let's bring in our data again. 

We are using our family Canada and US cities data.




---

```{python}
results_dict = {"depth": [], "mean_train_error": [], "mean_cv_error": [], "std_cv_error" : [], "std_train_error":[]}
param_grid = {"max_depth": np.arange(1, 20)}

for depth in param_grid["max_depth"]:
    model = DecisionTreeClassifier(max_depth=depth)
    scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)
    results_dict["depth"].append(depth)
    results_dict["mean_cv_error"].append(np.mean(1 - scores["test_score"]))
    results_dict["mean_train_error"].append(np.mean(1 - scores["train_score"]))
    results_dict["std_cv_error"].append((1-scores["test_score"]).std())
    results_dict["std_train_error"].append((1-scores["train_score"]).std())

results_df = pd.DataFrame(results_dict)
results_df = results_df.set_index("depth")
```

Notes: 

Here is a typical workflow to pick the best hyperparameters with a systematic search over some possible hyperparameter values.  

---

```{python}
results_df
```


Notes: 

---

```{python}
source = results_df.reset_index().melt(id_vars=['depth'] , 
                              value_vars=['mean_train_error', 'mean_cv_error'], 
                              var_name='plot', value_name='Error')
```


```{python}
chart1 = alt.Chart(source).mark_line().encode(
    alt.X('depth:Q', axis=alt.Axis(title="Tree Depth")),
    alt.Y('Error:Q'),
    alt.Color('plot:N', scale=alt.Scale(domain=['mean_train_error', 'mean_cv_error'],
                                           range=['teal', 'gold'])))
chart1
```

```{python include =FALSE}
chart1.save(path + 'chart1.png')
```

<img src="/module3/chart1.png" alt="A caption" width="65%" />




Notes: 

So which hyperparameter do we choose? 

There are many subtleties here and there is no perfect answer.

A common practice is to pick the model with minimum cross-validation error.

---

```{python}
results_df[results_df['mean_cv_error'] ==results_df['mean_cv_error'].min()]
best_depth = results_df[results_df['mean_cv_error'] ==results_df['mean_cv_error'].min()].index[0]
best_depth
```


```{python}
model = DecisionTreeClassifier(max_depth=best_depth)
model.fit(X_train, y_train);
print("Error on test set:" + str(round(1 - model.score(X_test, y_test))))
```

Notes: 

Let's pick `depth=5` which is where the mean cross-validation error is at a minimum. 

Let's now compare this error with the model's test error. 

Is the test error comparable with the cross-validation error?

Do we feel confident that this model would give a similar performance when deployed?



---

## The Golden Rule

Even though we care the most about test error:    
<center><b>THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY.</b></center> 

<br>

<img src="/module3/gavel.png" alt="A caption" width="90%" />

Notes: 

Even though we care the most about test error **THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY**. 

We have to be very careful not to violate it while developing our ML pipeline. 

Even experts end up breaking it sometimes which leads to misleading results and a lack of generalization on the real data. 

---

## Golden rule violation: Example 1  

<center><img src="/module3/golden_rule_violation.png" alt="A caption" width="58%" /></center>
<a href="https://www.theregister.com/2019/07/03/nature_study_earthquakes/" target="_blank">**Attribution: The A register - Katyanna Quach**</a>


Notes: 

*... He attempted to reproduce the research, and found a major flaw: there was some overlap in the data used to both train and test the model.*

---

## Golden rule violation: Example 2  

<center><img src="/module3/golden_rule_violation_2.png" alt="A caption" width="77%" /></center>
<a href="https://www.technologyreview.com/2015/06/04/72951/why-and-how-baidu-cheated-an-artificial-intelligence-test/" target="_blank">**Attribution:MIT Technology Review- Tom Simonite**</a>

Notes: 

*... The Challenge rules state that you must only test your code twice a week, because there‚Äôs an element of chance to the results. Baidu has admitted that it used multiple email accounts to test its code roughly 200 times in just under six months ‚Äì over four times what the rules allow.*

---

## How can we avoid violating the golden rule? 

<br>
<br>
<br>
<center>
<img src='/module3/train-test-split.png' alt="A caption" width="100%" />
</center>    

Notes: 

Recall that when we split data, we put our test set in an imaginary vault.


---

<br>
<br>

### Here is the workflow we'll generally follow. 

- **Splitting**: Before doing anything, split the data `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` or `train_df` and `test_df` using `train_test_split`.  
- **Select the best model using cross-validation**: Use `cross_validate` with `return_train_score = True` so that we can get access to training scores in each fold. (If we want to plot train vs validation error plots, for instance.) 
- **Scoring on test data**: Finally score on the test data with the chosen hyperparameters to examine the generalization performance.


Notes: 

Again, there are many subtleties here and we'll discuss the golden rule multiple times throughout the course and in the program.


---

# Let‚Äôs apply what we learned!

Notes: <br>