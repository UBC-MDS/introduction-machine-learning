---
params:
  dynamictitle: "module3_08"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module3/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module3/"
from display_tree import display_tree
```


type: slides

# Cross-validation

Notes: <br>

---

## Single split problems

<br>

<center><img src="/module3/train-valid-test-split.png"  width = "100%" alt="404 image" /></center>

Notes: 

We saw that it's necessary to split our data into multiple different sets/splits but is having a single train and validation split optimal? 

The problem with having a single train/validation split is that now we are using only a portion of our data for training and only a portion for validation. 

If our dataset is small we might end up with a tiny training and/or validation set.

We might also be unlucky with our splits such that they don't align well or don't well represent our test data.

---

## So what do we do? 


### ùëò-fold cross-validation

<center><img src="/module3/cross-validation.png"  width = "100%" alt="404 image" /></center>

Notes: 

We use something called ***cross-validation*** or ***ùëò-fold cross-validation*** as a solution to this problem.. 

Cross-validation helps us use all of our data for training/validation!

Cross-validation consists of splitting the data into k-folds ( ùëò>2 , often  ùëò=10 ).  In the picture below  ùëò=4 .

Each "fold" gets a turn at being the validation set.

Each fold gives a score and we usually average our ùëò results. 

It's better to notice the variation in the scores across folds.  

We can get a more "robust" measure of error on unseen data.

The main disadvantage here is that this is slower, which is a problem for bigger datasets / more complex models.


---

## Cross-validation using scikit-learn


```{python}
df = pd.read_csv("data/canada_usa_cities.csv")
X = df.drop(["country"], axis=1)
y = df["country"]
```

```{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123)
```


Notes: 

let's bring in our Canadian/United States cities data and split it.



---


```{python}
from sklearn.model_selection import cross_val_score

model = DecisionTreeClassifier(max_depth=4)
cv_score = cross_val_score(model, X_train, y_train, cv=5)
cv_score
```


Notes: 

First we import `cross_val_score` from `sklearn.model_selection`.

We create our decision tree model.

We use `cross_val_score()` and specify the model and the training features and target as arguments. We also specify `cv` which determines the cross-validation splitting strategy or how many "folds" there are.

Here we are saying there at 5 folds on the data. 

In each fold, it fits the model on the training portion and scores on the validation portion.

We can see the output is a list of validation scores in each fold.



---

```{python}
cross_val_score(model, X_train, y_train, cv=10)
```


Notes: 

We can change the number of folds too. Now, when we change it to 10, we get 10 different scores.

---

```{python}
from sklearn.model_selection import cross_validate
```

```{python}
scores = cross_validate(model, X, y, cv=10, return_train_score=True)
scores
```



Notes: 

`cross_val_score()` is the simpler scikit-learn function for cross-validation.

Let us access training and validation scores.  

`scores` is returned as a dictionary but it's much easier to understand if we convert it to a dataframe.


---

```{python}
pd.DataFrame(scores)
```

```{python}
pd.DataFrame(pd.DataFrame(scores).mean())
```

Notes: 

Using `cross_validate()` instead of `cross_val_score()` gives us more information. 

`cross_val_score()` was just returning that last column, but here we get the time spent and the training scores.

We can calculate the mean of each column on the 10 folds.

It's a bit unfortunate that they call it "test_score" in scikit-learn; for us this is a validation score.

---

### Our typical supervised learning set up is as follows: 

<br>

1. Given training data with `X` and `y`.
2. We split our data into `X_train, y_train, X_test, y_test`.
3. Hyperparameter optimization using cross-validation on `X_train` and `y_train`. 
4. We assess the best model using  `X_test` and `y_test`.
5. The **test error** tells us how well our model generalizes.
6. If the **test error** is reasonable, we deploy the model.


Notes: 
We are given training data with features `X` and target `y`.

We split the data into train and test portions: `X_train, y_train, X_test, y_test`.

We carry out hyperparameter optimization using cross-validation on the train portion: `X_train` and `y_train`. 

We assess our best performing model on the test portion: `X_test` and `y_test`.  
What we care about is the **test error**, which tells us how well our model can be generalized.

If this test error is "reasonable" we deploy the model which will be used on new unseen examples.

How do we know whether this test error is reasonable? We will discuss this in the next section!

---

# Let‚Äôs apply what we learned!

Notes: <br>
