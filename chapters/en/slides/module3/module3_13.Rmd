---
params:
  dynamictitle: "module3_13"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module3/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier
pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module3/"
from display_tree import display_tree
```


type: slides

# Overfitting and underfitting

Notes: <br>

---

We're going to think about 4 types of errors:

- **ğ¸_train**:  is our training error (or mean train error from cross-validation).
- **ğ¸_valid**  is our validation error (or mean validation error from cross-validation).
- **ğ¸_test**  is our test error.
- **ğ¸_best**  is the best possible error we could get for a given problem.

Question: Why is  ğ¸_best>0 ?



Notes: 

We've talked about the different types of splits but we've only briefly discussed error and the different types of error that we receive when building models. 

We saw in cross validation that there was train and validation error and image if they did not align with each other.

How do we diagnose the problem? 

We're going to think about 4 types of errors:

- ğ¸_train  is our training error (or mean train error from cross-validation).
- ğ¸_valid  is our validation error (or mean validation error from cross-validation).
-ğ¸_test  is our test error.
-ğ¸_best  is the best possible error we could get for a given problem.

---

```{python}
df = pd.read_csv("data/canada_usa_cities.csv")
X = df.drop(["country"], axis=1)
y = df["country"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123)
```

Notes: 

Let's bring back our Canadian and United States cities data to help explain the concepts of overfitting and underfitting.
 
---

## Overfitting


```python
model = DecisionTreeClassifier()
scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)
print("Train error:   %0.3f" % (1 - np.mean(scores["train_score"])))
print("Validation error:   %0.3f" % (1 - np.mean(scores["test_score"])))
```

```out
Train error:   0.000
Validation error:   0.191
```

```{python echo=FALSE, fig.width = 13, fig.height = 8,  out.width = '64%'}
model = DecisionTreeClassifier()
model.fit(X,y);
plt.figure(figsize=(4, 4))
plot_classifier(X.to_numpy(), y.to_numpy(), model)
plt.title("Decision tree with no max_depth")
```

Notes: 

Using decision tree with no specified max_depth, we can explain the phenomenon is called ***overfitting***. 

Overfitting is when our model fits the training data well and therefore the training error is low, however the model does not generalize to the validation set as well and the validation error is much higher. 

The Train error is low but validation error is much higher.

The gap between train and validation error is bigger.

A standard overfitting scenario would be: **ğ¸_train<ğ¸_best<ğ¸_valid**

If ğ¸_train is low, then we are in an overfitting scenario. It is fairly common to have at least a bit of this
 
ğ¸_valid  cannot be smaller than  ğ¸_best  basically by definition. In reality we won't have them equal.

---

## Underfitting 

```python
model = DecisionTreeClassifier(max_depth=1)

scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)
print("Train error: " + str(round(1 - np.mean(scores["train_score"]),2)))
print("Validation error: "  + str(round(1 - np.mean(scores["test_score"]),2)))
```

```out
Train error: 0.17
Validation error: 0.19
```

```{python echo=FALSE, fig.width = 13, fig.height = 8,  out.width = '64%'}

model = DecisionTreeClassifier(max_depth=1)
model.fit(X,y);
plt.figure(figsize=(4, 4))
plot_classifier(X.to_numpy(), y.to_numpy(), model)
plt.title("Decision tree with max_depth=1")
```


Notes: 

Using decision tree with a max_depth of 1, we can explain the phenomenon is called ***underfitting***. 

Underfitting is when our model is too simple (DecisionTreeClassifier with max_depth=1 or DummyClassifier). 

The model doesn't capture the patterns in the training data and the training error is not that low.

The model doesn't fit the data well and hence ğ¸_trainâ‰²ğ¸_valid.
 
Both train and validation errors are bad and the gap between train and validation error is lower.

**ğ¸_best<ğ¸_trainâ‰²ğ¸_valid**


---


<center><img src="/module3/over_under.png"  width = "50%" alt="404 image" /></center>



Standard question to ask ourself: 
***Which of these scenarios am I in?***

### How can we figure this out?

We can't see ğ¸_best but we can see ğ¸_train and ğ¸_test.

- If they are very far apart â†’ more likely **overfitting**.     
    - Try decreasing model complexity.


- If they are very close together â†’ underfitting regime.  
    - Try increasing model complexity.


Notes: 

<br>

---

# Letâ€™s apply what we learned!

Notes: <br>