---
params:
  dynamictitle: "module5_05"
title: "`r params$dynamictitle`"
output: 
  md_document:
    variant: gfm
---


```{r setup, include=FALSE}
## DO NOT FORGET TO CHANGE THIS ACCORDINGLY 
library(rmarkdown)
# MAke sure you are updating your title 
knitr::opts_chunk$set(echo = TRUE,
                      base.dir = ".", 
                      base.url = "/",
                      fig.path = paste("../../../../static/module5/", params$dynamictitle,"/", sep = ""))

knitr_opts <- knitr_options( opts_knit = NULL,
                             opts_chunk = NULL,
                             knit_hooks = NULL,
                             opts_hooks = NULL,
                             opts_template = NULL)
md_document_custom <- md_document(variant = "gfm")
output_format(knitr = knitr_opts,
              pandoc = NULL,
              base_format = md_document_custom)
library(reticulate)

```


```{python include=FALSE}
import pandas as pd
import numpy as np
import altair as alt
from altair_saver import save
import glob
from sklearn.pipeline import Pipeline, make_pipeline
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz

from IPython.display import HTML, display
from PIL import Image, ImageFile

from plot_classifier import plot_classifier


# Classifiers and regressors
from sklearn.dummy import DummyClassifier, DummyRegressor

# Preprocessing and pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import euclidean_distances

# train test split and cross validation
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


pd.set_option('display.width', 350)

np.set_printoptions(linewidth=400)

pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 15)

path = "../../../../static/module5/"
```


type: slides

# Preprocessing with imputation

Notes: <br>

---

## Case study: California housing prices

```{python}
housing_df = pd.read_csv("data/housing.csv")
train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)

train_df.head()
```
 We are using the data that can be <a href="https://www.kaggle.com/harrywang/housing" target="_blank">downloaded here</a>. 
 
This dataset is a modified version of the California Housing dataset available from: <a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html" target="_blank">Luís Torgo's University of Porto website</a>.

 
 
 
 
Notes: 

For the next few slide decks, we are going to be using a dataset exploring the prices of homes in California to demonstrate feature transformation techniques. 

The task is to predict median house values in Californian districts, given several features from these districts. 

We can see here that some column values are mean/median while others are totals or not completely clear. 

---

```{python}
train_df = train_df.assign(rooms_per_household = train_df["total_rooms"]/train_df["households"],
                           bedrooms_per_household = train_df["total_bedrooms"]/train_df["households"],
                           population_per_household = train_df["population"]/train_df["households"])
                        
test_df = test_df.assign(rooms_per_household = test_df["total_rooms"]/test_df["households"],
                         bedrooms_per_household = test_df["total_bedrooms"]/test_df["households"],
                         population_per_household = test_df["population"]/test_df["households"])
                         
train_df = train_df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])  
test_df = test_df.drop(columns=['total_rooms', 'total_bedrooms', 'population']) 

train_df.head()
```

Notes: 

Let's add some new features to the dataset which could help predict the target: `median_house_value`.

**When is it OK to do things before splitting?**

Here it would have been OK to add new features before splitting because we are not using any global information in the data but only looking at one row at a time. 

But just to be safe and to avoid accidentally breaking the golden rule, it's better to do it after splitting. 

Next, we remove the columns `total_rooms`, `total_bedrooms`, and `population`.


---

## Exploratory Data Analysis (EDA)

```{python}
train_df.info()
```

Notes: 

Here we see that we have all columns with dtype `float64` except for `ocean_proximity` which appears categorical. 

It looks like the `bedrooms_per_household` columns have some missing values.

---

 
 
```{python}
train_df.describe()
```
```{python}
train_df["bedrooms_per_household"].isnull().sum()

```

Notes: 

Looks like the training data is missing 185 values for `bedrooms_per_household`.

---

### What happens?

```{python}
X_train = train_df.drop(columns=["median_house_value", "ocean_proximity"])
y_train = train_df["median_house_value"]

X_test = test_df.drop(columns=["median_house_value", "ocean_proximity"])
y_test = test_df["median_house_value"]
```

```python
knn = KNeighborsRegressor()
knn.fit(X_train, y_train)
```
```out
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
```

Notes: 

First, we are going to drop the categorical variable `ocean_proximity`.  

Right now, we only know how to build models with numerical data. We will come back to categorical variables in module 6. 

But what do we do about the null values? Can we run a model with them? 

We can see that the classifier is not able to deal with missing values (NaNs).

What are the possible ways to deal with the problem? 

---

## Dropping

```{python}
train_df["bedrooms_per_household"].isnull().sum()
```

```{python}
X_train.shape
```

```{python}
X_train_no_nan = X_train.dropna()
y_train_no_nan = y_train.dropna()
```

```{python}
X_train_no_nan.shape
```

 
 
Notes: 

We could drop the rows but we'd need to do the same in our test set.

That also doesn't help us if we get a missing value in deployment. What do we do then? 

Furthermore, what if the missing values don't occur at random and we're systematically dropping certain data?

This is not a great solution, especially if there's a lot of missing values.

---

## Dropping a column

```{python}
X_train.shape
```

```{python}
X_train_no_col = X_train.dropna(axis=1)
```

```{python}
X_train_no_col.shape
```

Notes:

One can also drop all columns with missing values.

This generally throws away a lot of information, because we lose a whole column just for 1 missing value.
But I might drop a column if it's 99.9% missing values, for example.

---

## Imputation

**Imputation**: Imputation means inventing values for the missing data.

```{python}
from sklearn.impute import SimpleImputer
```

We can impute missing values in:

- **Categorical columns**: with the most frequent value.
- **Numeric columns**: with the mean or median of the column or a constant of our choosing.

Notes: 

`SimpleImputer` is a **transformer** in `sklearn` which can deal with this problem. 

We are going to concentrate on numeric columns in this section and address categorical preprocessing in Module 6. 

---

```{python}
X_train.sort_values('bedrooms_per_household').tail(10)
```

Notes: 

Let's take a look at some of the examples with `NaN` values in `total_bedrooms`. 

Here we see that the index `7763` has a `NaN` value for `total_bedrooms` and `bedrooms_per_household`.

---

```{python}
imputer = SimpleImputer(strategy="median")
imputer.fit(X_train);
X_train_imp = imputer.transform(X_train)
X_test_imp = imputer.transform(X_test)
```

```{python}
X_train_imp
```

 
 
Notes: 

Let's input our data and instead of dropping the examples, let’s use the `fit` and `transform` steps that we saw earlier.

We do not need to fit on our target column this time. 

Note that `imputer.transform` returns a NumPy array and not a dataframe.

---

```{python}
X_train_imp_df = pd.DataFrame(X_train_imp, columns = X_train.columns, index = X_train.index)
X_train_imp_df.loc[[7763]]
```
```{python}
X_train.loc[[7763]]
```

Notes: 

Let's check whether the `NaN` values have been replaced or not.

We can fix that a bit using the column index from `X_train` and transforming it into a dataframe. 

Now we can see our example 7763 no longer has any `NaN` values for `total_bedrooms` and `bedrooms_per_household`. 

---

```{python}
knn = KNeighborsRegressor();
knn.fit(X_train_imp, y_train)
knn.score(X_train_imp, y_train)
```

Notes: 

Can we train on the data with the new data `X_train_imp`?  

Yes! 

---

# Let’s apply what we learned!

Notes: <br>