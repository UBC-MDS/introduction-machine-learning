---
type: slides
---

# Case Study: Preprocessing with Scaling

Notes: <br>

---

``` python
knn_unscaled = KNeighborsClassifier();
knn_unscaled.fit(X_train, y_train)
print('Train score: ', (knn_unscaled.score(X_train, y_train).round(2)))
print('Test score: ', (knn_unscaled.score(X_test, y_test).round(2)))
```

``` out
KNeighborsClassifier()
Train score:  0.71
Test score:  0.45
```

``` python
knn_scaled = KNeighborsClassifier();
knn_scaled.fit(X_train_scaled, y_train)
print('Train score: ', (knn_scaled.score(X_train_scaled, y_train).round(2)))
print('Test score: ', (knn_scaled.score(X_test_scaled, y_test).round(2)))
```

``` out
KNeighborsClassifier()
Train score:  0.94
Test score:  0.89
```

Notes:

We‚Äôve seen why scaling is important when we were using our basketball
dataset in the first section of this module.

In this section, we are going to dive a little deeper into the process
and the transformer options.

---

## Scaling

<center>

<img src="/module5/scaling-data.png"  width = "75%" alt="404 image" />

</center>

<a href="https://amueller.github.io/COMS4995-s19/slides/aml-05-preprocessing/#8" target="_blank">Attribution</a>

Notes:

This problem affects a large number of ML methods.

There are a number of approaches to this problem.

We are going to look at two popular ones; `MinMaxScaler` and
`StandardScaler`.

---

| Approach        | What it does                     | How to update ùëã (but see below\!)                      | sklearn implementation                                                                                                                                                            |
| --------------- | -------------------------------- | ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Normalization   | sets range to \[0,1\]            | `X -= np.min(X, axis=0)`<br>`X /= np.max(X, axis=0)`   | <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" target="_blank">`MinMaxScaler()`</a>                                          |
| Standardization | sets sample mean to 0, s.d. to 1 | `X -= np.mean(X, axis=0)`<br>`X /=  np.std(X, axis=0)` | <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" target="_blank">`StandardScaler()`</a> |

There are all sorts of articles on this: see
<a href="http://www.dataminingblog.com/standardization-vs-normalization/" target="_blank">here</a>
and
<a href="https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc" target="_blank">here</a>.

Notes:

<br>

---

``` python
pd.DataFrame(X_train_imp, columns=X_train.columns, index=X_train.index).head()
```

```out
       longitude  latitude  housing_median_age  households  median_income  rooms_per_household  bedrooms_per_household  population_per_household
6051     -117.75     34.04                22.0       602.0         3.1250             4.897010                1.056478                  4.318937
20113    -119.57     37.94                17.0        20.0         3.4861            17.300000                6.500000                  2.550000
14289    -117.13     32.74                46.0       708.0         2.6604             4.738701                1.084746                  2.057910
13665    -117.31     34.02                18.0       285.0         5.2139             5.733333                0.961404                  3.154386
14471    -117.23     32.88                18.0      1458.0         1.8580             3.817558                1.004801                  4.323045
```

``` python
from sklearn.preprocessing import StandardScaler
```

``` python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imp)
X_test_scaled = scaler.transform(X_test_imp)
pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index).head()
```

```out
       longitude  latitude  housing_median_age  households  median_income  rooms_per_household  bedrooms_per_household  population_per_household
6051    0.908140 -0.743917           -0.526078    0.266135      -0.389736            -0.210591               -0.083813                  0.126398
20113  -0.002057  1.083123           -0.923283   -1.253312      -0.198924             4.726412               11.166631                 -0.050132
14289   1.218207 -1.352930            1.380504    0.542873      -0.635239            -0.273606               -0.025391                 -0.099240
13665   1.128188 -0.753286           -0.843842   -0.561467       0.714077             0.122307               -0.280310                  0.010183
14471   1.168196 -1.287344           -0.843842    2.500924      -1.059242            -0.640266               -0.190617                  0.126808
```

Notes: Let‚Äôs bring in our imputed data that we processed in the last
slide deck.

We‚Äôve seen the `StandardScaler()` function earlier but let‚Äôs see what
the transformed data looks like.

Now we can compare our training score with scaled and unscaled data and
see a noticeable difference between the two\!

---

**Unscaled data**

``` python
knn = KNeighborsRegressor()
knn.fit(X_train_imp, y_train);
print(knn.score(X_train_imp, y_train).round(3))
```

``` python
print(knn.score(X_test_imp, y_test).round(3))
```

``` out
0.561
0.32
```

**Scaled data**

``` python
knn = KNeighborsRegressor()
knn.fit(X_train_scaled, y_train);
print(knn.score(X_train_scaled, y_train).round(3))
```

``` python
print(knn.score(X_test_scaled, y_test).round(3))
```

``` out
0.798
0.712
```

Notes:

How about our training scores?

Just using scaled column values increases the training score by 30%\!

---

``` python
from sklearn.preprocessing import MinMaxScaler
```

``` python
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_imp)
X_test_scaled = scaler.transform(X_test_imp)
pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index).head()
```

```out
       longitude  latitude  housing_median_age  households  median_income  rooms_per_household  bedrooms_per_household  population_per_household
6051    0.657371  0.159405            0.411765    0.098832       0.181039             0.028717                0.021437                  0.002918
20113   0.476096  0.573858            0.313725    0.003124       0.205942             0.116642                0.182806                  0.001495
14289   0.719124  0.021254            0.882353    0.116264       0.148998             0.027594                0.022275                  0.001099
13665   0.701195  0.157279            0.333333    0.046703       0.325099             0.034645                0.018619                  0.001981
14471   0.709163  0.036132            0.333333    0.239599       0.093661             0.021064                0.019905                  0.002922
```

Notes:

Let‚Äôs now explore `MinMaxScaler`.

Looking at the data after transforming it with `MinMaxScaler()` we see
this time there are no negative values.

---

**Unscaled data**

``` python
knn = KNeighborsRegressor()
knn.fit(X_train_imp, y_train);
print(knn.score(X_train_imp, y_train).round(3))
```

``` python
print(knn.score(X_test_imp, y_test).round(3))
```

``` out
0.561
0.32
```

**Scaled data**

``` python
knn = KNeighborsRegressor()
knn.fit(X_train_scaled, y_train);
print(knn.score(X_train_scaled, y_train).round(3))
```

``` python
print(knn.score(X_test_scaled, y_test).round(3))
```

``` out
0.801
0.723
```

Notes:

Again, similar to `StandardScaler` there is a big difference in the KNN
training performance after scaling the data.

But we saw last week that the training score doesn‚Äôt tell us much.

We should look at the cross-validation score.

Let‚Äôs take a look at that in the next section.

---

# Let‚Äôs apply what we learned\!

Notes: <br>
